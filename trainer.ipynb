{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 13:56:21.421986: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733732781.442067  115641 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733732781.447540  115641 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-09 13:56:21.468376: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Llama Tokenizer - Step 1\n",
    "from transformers import MimiModel, AutoFeatureExtractor, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TextTokenizer:\n",
    "    def __init__(self, name='Llama_tokenizer'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(name, legacy=False)\n",
    "        print(\"text vocab size\", self.tokenizer.vocab_size)\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return self.tokenizer.decode(tokens)\n",
    "    \n",
    "class MimiTokenizer:\n",
    "    def __init__(self, device):    \n",
    "        self.device = device\n",
    "        self.model = MimiModel.from_pretrained(\"kyutai/mimi\")\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(\"kyutai/mimi\", device=device)\n",
    "        self.sampling_rate = self.feature_extractor.sampling_rate\n",
    "        self.n_codebooks = 8\n",
    "        self.vocab_size = 2048\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def encode(self, waveform):\n",
    "        inputs = self.feature_extractor(raw_audio=waveform, \n",
    "                                        sampling_rate=self.sampling_rate, \n",
    "                                        return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "        output = self.model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"], num_quantizers=self.n_codebooks)\n",
    "        tokens = output.audio_codes[0].cpu().numpy()\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        assert len(tokens.shape) == 2\n",
    "        tokens = torch.tensor(np.expand_dims(tokens, axis=0)).to(self.device)\n",
    "        output = self.model.decode(tokens)\n",
    "        waveform = output.audio_values.cpu()\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text vocab size 128000\n",
      "Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition\n",
      "[128000, 39628, 11, 304, 279, 1193, 5647, 449, 902, 584, 527, 520, 3118, 11920, 11, 44642, 505, 1455, 422, 539, 505, 682, 279, 19071, 323, 44948, 15609, 304, 279, 68033]\n",
      "<|begin_of_text|>Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "tokenizer = TextTokenizer()\n",
    "with open('/home/subhash/.cache/indri/lj_speech/annotation/metadata.jsonl') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        text = data['raw_text']\n",
    "        tokens = tokenizer.encode(text)\n",
    "        print(text)\n",
    "        print(tokens)\n",
    "        print(tokenizer.decode(tokens))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mimi Tokeniser - Step 2\n",
    "import numpy as np\n",
    "mimi_tokens = np.load(\"/home/subhash/.cache/indri/lj_speech/tokens/mimi/LJ040-0046.npy\")\n",
    "print(mimi_tokens.shape)\n",
    "#print(mimi_tokens)\n",
    "type(mimi_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([128995, 130325, 132834, 135350, 138081, 138819, 140988, 143449,\n",
       "       128572, 131272, 133038, 135276, 136552, 139416, 141865, 142522,\n",
       "       129117, 131597, 133406, 134146, 136322, 139289, 141923, 143985,\n",
       "       129069, 131186, 133662, 135220, 136512, 139853, 141760, 142410,\n",
       "       129620, 131843, 133121, 134814, 137436, 138387, 141084, 142444,\n",
       "       128080, 131002, 133038, 135147, 137077, 139665, 141897, 142752,\n",
       "       129134, 131203, 132924, 134668, 136548, 140002, 140409, 143451,\n",
       "       128715, 130220, 133849, 135225, 136357, 139775, 140460, 143596,\n",
       "       129650, 131937, 132126, 134545, 136933, 139077, 141937, 143864,\n",
       "       129532, 131639, 132479, 134835, 137883, 138246, 141974, 143201,\n",
       "       128937, 131824, 133411, 134384, 136248, 138246, 141565, 143949,\n",
       "       129077, 130323, 132375, 134147, 138069, 139010, 141512, 142954,\n",
       "       129352, 130386, 133860, 134608, 138148, 140177, 141688, 143239,\n",
       "       128749, 130955, 133781, 135938, 137985, 140133, 141270, 143942,\n",
       "       128577, 130291, 133245, 134258, 137928, 138587, 141264, 142904,\n",
       "       128670, 131159, 132503, 135148, 136914, 139760, 140828, 142475,\n",
       "       129701, 130488, 133298, 134413, 136709, 138498, 140735, 142696,\n",
       "       129762, 131203, 132992, 136027, 136402, 138246, 141713, 142997,\n",
       "       128570, 131397, 133382, 134994, 136195, 139013, 142075, 142487,\n",
       "       129468, 130744, 133054, 135043, 136663, 140173, 141663, 142607,\n",
       "       128157, 132044, 132786, 135857, 136657, 138778, 140733, 142574,\n",
       "       128839, 131041, 133885, 134696, 136357, 139132, 140508, 144299,\n",
       "       129457, 131094, 132982, 135497, 137717, 140221, 141218, 142767,\n",
       "       128181, 131341, 133840, 135517, 136861, 140137, 141400, 142794,\n",
       "       129022, 131672, 132922, 135561, 137960, 138869, 140476, 143155,\n",
       "       129037, 130437, 132452, 134366, 136541, 138498, 140552, 143492,\n",
       "       129067, 131891, 132160, 135811, 137641, 138246, 140940, 143208,\n",
       "       129132, 130387, 133891, 136062, 137641, 139273, 141345, 143673,\n",
       "       129320, 131104, 133274, 134286, 137706, 138808, 141526, 143887,\n",
       "       129607, 130291, 133245, 134258, 137207, 138795, 142159, 143942,\n",
       "       129607, 131259, 133245, 134434, 136773, 139812, 141797, 143942,\n",
       "       128663, 130291, 133655, 135492, 137107, 139683, 141526, 144344,\n",
       "       129779, 130291, 133655, 135492, 136773, 139683, 142159, 144344,\n",
       "       129301, 130291, 133655, 135492, 136459, 139683, 141526, 144080,\n",
       "       129618, 131765, 133686, 135920, 136515, 138568, 140382, 142654,\n",
       "       129782, 130680, 134088, 135388, 138099, 139164, 141007, 142836,\n",
       "       128304, 130689, 133573, 135245, 138128, 140000, 141169, 144180,\n",
       "       129219, 131390, 134066, 134549, 136840, 139839, 142026, 143399,\n",
       "       129640, 130582, 133627, 134822, 136510, 138754, 141771, 143770,\n",
       "       129196, 130787, 134070, 136099, 136409, 138550, 142227, 142524,\n",
       "       128505, 130783, 133420, 135114, 137860, 140104, 140948, 144171,\n",
       "       129146, 130063, 132779, 135688, 137530, 139481, 141406, 142520,\n",
       "       128784, 131671, 132450, 136045, 137666, 138890, 141700, 143080,\n",
       "       128917, 131754, 132133, 134842, 137151, 139775, 142253, 143311,\n",
       "       129708, 130779, 133640, 135447, 136465, 139175, 141048, 142408,\n",
       "       128922, 130386, 132886, 134893, 136343, 139580, 141355, 142594,\n",
       "       129137, 130821, 132533, 135072, 136226, 139759, 140997, 143102,\n",
       "       129358, 130868, 133712, 134398, 137393, 139796, 140534, 143363,\n",
       "       128126, 130538, 132762, 134984, 136773, 139979, 140954, 143887,\n",
       "       128832, 131259, 133793, 135492, 137928, 139809, 141526, 144080,\n",
       "       129743, 130291, 133655, 135492, 137928, 139809, 141113, 144344,\n",
       "       128663, 130291, 133245, 134308, 136773, 139270, 142159, 143942,\n",
       "       129544, 131104, 132879, 135492, 137928, 139812, 141113, 144344,\n",
       "       128835, 130941, 132602, 135049, 136666, 139613, 140860, 144269,\n",
       "       128337, 131187, 133089, 134987, 136678, 139449, 140465, 143899,\n",
       "       129377, 131817, 133781, 134413, 136337, 140221, 141484, 143470,\n",
       "       130024, 130741, 133627, 134670, 137965, 138514, 141771, 142828,\n",
       "       129252, 130240, 133686, 134497, 138092, 140015, 142160, 143434,\n",
       "       129260, 130871, 133216, 135834, 137580, 139135, 141863, 142524,\n",
       "       128304, 130417, 132762, 135214, 138040, 139256, 141175, 142728,\n",
       "       129039, 131825, 133669, 135608, 138197, 138794, 140372, 143522,\n",
       "       129640, 130696, 133360, 136125, 138013, 138913, 141370, 142998,\n",
       "       128797, 131259, 133598, 134286, 136459, 138587, 141526, 143749,\n",
       "       128262, 131259, 133655, 135492, 136532, 139809, 141526, 144080,\n",
       "       128663, 130291, 133655, 135492, 137928, 139809, 142266, 144344,\n",
       "       128384, 131397, 132762, 135841, 138197, 138794, 141281, 144204,\n",
       "       130025, 130837, 133686, 134666, 137959, 139039, 141433, 143179,\n",
       "       129774, 130979, 133778, 134741, 137010, 138806, 141633, 143066,\n",
       "       128178, 130680, 133776, 134877, 136409, 139577, 140681, 143858,\n",
       "       128129, 131251, 133595, 135833, 136396, 138246, 140669, 142598,\n",
       "       129669, 130333, 132352, 134878, 136717, 138246, 141089, 143782,\n",
       "       128421, 130505, 132348, 134838, 137561, 138741, 142305, 143603,\n",
       "       129519, 131958, 132468, 135590, 136966, 139421, 141476, 142660,\n",
       "       129022, 130951, 133706, 135882, 136521, 139121, 141580, 143632,\n",
       "       129562, 130376, 132313, 134835, 137641, 138246, 140703, 143165,\n",
       "       129252, 131627, 133411, 134736, 136208, 139572, 141000, 142947,\n",
       "       129782, 130468, 133482, 134796, 136859, 140087, 141757, 143059,\n",
       "       128304, 131566, 133203, 135172, 136604, 139309, 141337, 143786,\n",
       "       129882, 130079, 133471, 135332, 136465, 139874, 142303, 142508,\n",
       "       129721, 130225, 133411, 135207, 137798, 138246, 141517, 143754,\n",
       "       129356, 131988, 133442, 135049, 137706, 139295, 141301, 143426,\n",
       "       128096, 131763, 133583, 135882, 138042, 139177, 140579, 143748,\n",
       "       128777, 132076, 133283, 135913, 136326, 139039, 140993, 143849,\n",
       "       128505, 131396, 133917, 135805, 136482, 139801, 140598, 143465,\n",
       "       128526, 131624, 134083, 135933, 137119, 138807, 142219, 143542,\n",
       "       129146, 131526, 133208, 134889, 136540, 138609, 140881, 143960,\n",
       "       129235, 130842, 134120, 135056, 136239, 139371, 142065, 143465,\n",
       "       128981, 131914, 132919, 136140, 137531, 139670, 140380, 143349,\n",
       "       129708, 130302, 132534, 136098, 137274, 138754, 142050, 143112,\n",
       "       129473, 130871, 133060, 136084, 137224, 139800, 141985, 142724,\n",
       "       129358, 130417, 132419, 134201, 137032, 139209, 142248, 144188,\n",
       "       128502, 131104, 133722, 134308, 136773, 139270, 141800, 144344,\n",
       "       129268, 131552, 132208, 134758, 137754, 138645, 140963, 143949,\n",
       "       128885, 130731, 133042, 134676, 138067, 138689, 141153, 142518,\n",
       "       129558, 130562, 132762, 135147, 136479, 138505, 141154, 142954,\n",
       "       129252, 130302, 133623, 135723, 136773, 138820, 141661, 142916,\n",
       "       129260, 132080, 132886, 135635, 137689, 140076, 141626, 143059,\n",
       "       128304, 132035, 132754, 135963, 137388, 139734, 140330, 143591,\n",
       "       129631, 130914, 133663, 134275, 137724, 139944, 141107, 142904,\n",
       "       129565, 130486, 132722, 136154, 136729, 138547, 141040, 142487])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weave Tokens, codebook offset - Step 3\n",
    "def weave_tokens(tokens):\n",
    "    result = []\n",
    "    max_length = max(len(codebook) for codebook in tokens)\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        for codebook_index, codebook in enumerate(tokens):\n",
    "            if i < len(codebook):\n",
    "                offset = 2048 * codebook_index + 128000\n",
    "                result.append(codebook[i] + offset)                 \n",
    "    return np.array(result)\n",
    "\n",
    "weave_audio = weave_tokens(mimi_tokens.tolist())\n",
    "weave_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class TTSTokenizer:\n",
    "    def __init__(self, text_tokenizer_name='tts_tokenizer', audio_tokenizer_name='tts_tokenizer'):\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(text_tokenizer_name, legacy=False)\n",
    "        self.audio_tokenizer = AutoTokenizer.from_pretrained(audio_tokenizer_name, legacy=False)\n",
    "\n",
    "    def encode(self, input_data, add_special_tokens=True):\n",
    "        if isinstance(input_data, str):\n",
    "            encoded_tokens = self.text_tokenizer.encode(\n",
    "                input_data, \n",
    "                return_tensors='pt', \n",
    "                add_special_tokens=add_special_tokens\n",
    "            )\n",
    "            return encoded_tokens\n",
    "        elif isinstance(input_data, list) and all(isinstance(item, str) for item in input_data):\n",
    "            encoded_tokens = self.audio_tokenizer.encode(\n",
    "                input_data, \n",
    "                return_tensors='pt', \n",
    "                add_special_tokens=add_special_tokens\n",
    "            )\n",
    "            return encoded_tokens\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a string or a list of strings\")\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        if not isinstance(tokens, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch tensor of tokens\")\n",
    "        \n",
    "        try:\n",
    "            decoded_text = self.text_tokenizer.decode(tokens)\n",
    "            return decoded_text\n",
    "        except:\n",
    "            try:\n",
    "                decoded_tokens = self.audio_tokenizer.decode(tokens)\n",
    "                return torch.tensor(decoded_tokens)\n",
    "            except:\n",
    "                raise ValueError(\"Unable to decode the provided tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition\n",
      "[aco_739][aco_2069][aco_4578][aco_7094][aco_9825][aco_10563][aco_12732][aco_15193][aco_316][aco_3016][aco_4782][aco_7020][aco_8296][aco_11160][aco_13609][aco_14266][aco_861][aco_3341][aco_5150][aco_5890][aco_8066][aco_11033][aco_13667][aco_15729][aco_813][aco_2930][aco_5406][aco_6964][aco_8256][aco_11597][aco_13504][aco_14154][aco_1364][aco_3587][aco_4865][aco_6558][aco_9180][aco_10131][aco_12828][aco_14188]<|reserved_special_token_72|>[aco_2746][aco_4782][aco_6891][aco_8821][aco_11409][aco_13641][aco_14496][aco_878][aco_2947][aco_4668][aco_6412][aco_8292][aco_11746][aco_12153][aco_15195][aco_459][aco_1964][aco_5593][aco_6969][aco_8101][aco_11519][aco_12204][aco_15340][aco_1394][aco_3681][aco_3870][aco_6289][aco_8677][aco_10821][aco_13681][aco_15608][aco_1276][aco_3383][aco_4223][aco_6579][aco_9627][aco_9990][aco_13718][aco_14945][aco_681][aco_3568][aco_5155][aco_6128][aco_7992][aco_9990][aco_13309][aco_15693][aco_821][aco_2067][aco_4119][aco_5891][aco_9813][aco_10754][aco_13256][aco_14698][aco_1096][aco_2130][aco_5604][aco_6352][aco_9892][aco_11921][aco_13432][aco_14983][aco_493][aco_2699][aco_5525][aco_7682][aco_9729][aco_11877][aco_13014][aco_15686][aco_321][aco_2035][aco_4989][aco_6002][aco_9672][aco_10331][aco_13008][aco_14648][aco_414][aco_2903][aco_4247][aco_6892][aco_8658][aco_11504][aco_12572][aco_14219][aco_1445][aco_2232][aco_5042][aco_6157][aco_8453][aco_10242][aco_12479][aco_14440][aco_1506][aco_2947][aco_4736][aco_7771][aco_8146][aco_9990][aco_13457][aco_14741][aco_314][aco_3141][aco_5126][aco_6738][aco_7939][aco_10757][aco_13819][aco_14231][aco_1212][aco_2488][aco_4798][aco_6787][aco_8407][aco_11917][aco_13407][aco_14351]<|reserved_special_token_149|>[aco_3788][aco_4530][aco_7601][aco_8401][aco_10522][aco_12477][aco_14318][aco_583][aco_2785][aco_5629][aco_6440][aco_8101][aco_10876][aco_12252][aco_16043][aco_1201][aco_2838][aco_4726][aco_7241][aco_9461][aco_11965][aco_12962][aco_14511]<|reserved_special_token_173|>[aco_3085][aco_5584][aco_7261][aco_8605][aco_11881][aco_13144][aco_14538][aco_766][aco_3416][aco_4666][aco_7305][aco_9704][aco_10613][aco_12220][aco_14899][aco_781][aco_2181][aco_4196][aco_6110][aco_8285][aco_10242][aco_12296][aco_15236][aco_811][aco_3635][aco_3904][aco_7555][aco_9385][aco_9990][aco_12684][aco_14952][aco_876][aco_2131][aco_5635][aco_7806][aco_9385][aco_11017][aco_13089][aco_15417][aco_1064][aco_2848][aco_5018][aco_6030][aco_9450][aco_10552][aco_13270][aco_15631][aco_1351][aco_2035][aco_4989][aco_6002][aco_8951][aco_10539][aco_13903][aco_15686][aco_1351][aco_3003][aco_4989][aco_6178][aco_8517][aco_11556][aco_13541][aco_15686][aco_407][aco_2035][aco_5399][aco_7236][aco_8851][aco_11427][aco_13270][aco_16088][aco_1523][aco_2035][aco_5399][aco_7236][aco_8517][aco_11427][aco_13903][aco_16088][aco_1045][aco_2035][aco_5399][aco_7236][aco_8203][aco_11427][aco_13270][aco_15824][aco_1362][aco_3509][aco_5430][aco_7664][aco_8259][aco_10312][aco_12126][aco_14398][aco_1526][aco_2424][aco_5832][aco_7132][aco_9843][aco_10908][aco_12751][aco_14580][aco_48][aco_2433][aco_5317][aco_6989][aco_9872][aco_11744][aco_12913][aco_15924][aco_963][aco_3134][aco_5810][aco_6293][aco_8584][aco_11583][aco_13770][aco_15143][aco_1384][aco_2326][aco_5371][aco_6566][aco_8254][aco_10498][aco_13515][aco_15514][aco_940][aco_2531][aco_5814][aco_7843][aco_8153][aco_10294][aco_13971][aco_14268][aco_249][aco_2527][aco_5164][aco_6858][aco_9604][aco_11848][aco_12692][aco_15915][aco_890][aco_1807][aco_4523][aco_7432][aco_9274][aco_11225][aco_13150][aco_14264][aco_528][aco_3415][aco_4194][aco_7789][aco_9410][aco_10634][aco_13444][aco_14824][aco_661][aco_3498][aco_3877][aco_6586][aco_8895][aco_11519][aco_13997][aco_15055][aco_1452][aco_2523][aco_5384][aco_7191][aco_8209][aco_10919][aco_12792][aco_14152][aco_666][aco_2130][aco_4630][aco_6637][aco_8087][aco_11324][aco_13099][aco_14338][aco_881][aco_2565][aco_4277][aco_6816][aco_7970][aco_11503][aco_12741][aco_14846][aco_1102][aco_2612][aco_5456][aco_6142][aco_9137][aco_11540][aco_12278][aco_15107]<|reserved_special_token_118|>[aco_2282][aco_4506][aco_6728][aco_8517][aco_11723][aco_12698][aco_15631][aco_576][aco_3003][aco_5537][aco_7236][aco_9672][aco_11553][aco_13270][aco_15824][aco_1487][aco_2035][aco_5399][aco_7236][aco_9672][aco_11553][aco_12857][aco_16088][aco_407][aco_2035][aco_4989][aco_6052][aco_8517][aco_11014][aco_13903][aco_15686][aco_1288][aco_2848][aco_4623][aco_7236][aco_9672][aco_11556][aco_12857][aco_16088][aco_579][aco_2685][aco_4346][aco_6793][aco_8410][aco_11357][aco_12604][aco_16013][aco_81][aco_2931][aco_4833][aco_6731][aco_8422][aco_11193][aco_12209][aco_15643][aco_1121][aco_3561][aco_5525][aco_6157][aco_8081][aco_11965][aco_13228][aco_15214][aco_1768][aco_2485][aco_5371][aco_6414][aco_9709][aco_10258][aco_13515][aco_14572][aco_996][aco_1984][aco_5430][aco_6241][aco_9836][aco_11759][aco_13904][aco_15178][aco_1004][aco_2615][aco_4960][aco_7578][aco_9324][aco_10879][aco_13607][aco_14268][aco_48][aco_2161][aco_4506][aco_6958][aco_9784][aco_11000][aco_12919][aco_14472][aco_783][aco_3569][aco_5413][aco_7352][aco_9941][aco_10538][aco_12116][aco_15266][aco_1384][aco_2440][aco_5104][aco_7869][aco_9757][aco_10657][aco_13114][aco_14742][aco_541][aco_3003][aco_5342][aco_6030][aco_8203][aco_10331][aco_13270][aco_15493][aco_6][aco_3003][aco_5399][aco_7236][aco_8276][aco_11553][aco_13270][aco_15824][aco_407][aco_2035][aco_5399][aco_7236][aco_9672][aco_11553][aco_14010][aco_16088][aco_128][aco_3141][aco_4506][aco_7585][aco_9941][aco_10538][aco_13025][aco_15948][aco_1769][aco_2581][aco_5430][aco_6410][aco_9703][aco_10783][aco_13177][aco_14923][aco_1518][aco_2723][aco_5522][aco_6485][aco_8754][aco_10550][aco_13377][aco_14810]<|reserved_special_token_170|>[aco_2424][aco_5520][aco_6621][aco_8153][aco_11321][aco_12425][aco_15602]<|reserved_special_token_121|>[aco_2995][aco_5339][aco_7577][aco_8140][aco_9990][aco_12413][aco_14342][aco_1413][aco_2077][aco_4096][aco_6622][aco_8461][aco_9990][aco_12833][aco_15526][aco_165][aco_2249][aco_4092][aco_6582][aco_9305][aco_10485][aco_14049][aco_15347][aco_1263][aco_3702][aco_4212][aco_7334][aco_8710][aco_11165][aco_13220][aco_14404][aco_766][aco_2695][aco_5450][aco_7626][aco_8265][aco_10865][aco_13324][aco_15376][aco_1306][aco_2120][aco_4057][aco_6579][aco_9385][aco_9990][aco_12447][aco_14909][aco_996][aco_3371][aco_5155][aco_6480][aco_7952][aco_11316][aco_12744][aco_14691][aco_1526][aco_2212][aco_5226][aco_6540][aco_8603][aco_11831][aco_13501][aco_14803][aco_48][aco_3310][aco_4947][aco_6916][aco_8348][aco_11053][aco_13081][aco_15530][aco_1626][aco_1823][aco_5215][aco_7076][aco_8209][aco_11618][aco_14047][aco_14252][aco_1465][aco_1969][aco_5155][aco_6951][aco_9542][aco_9990][aco_13261][aco_15498][aco_1100][aco_3732][aco_5186][aco_6793][aco_9450][aco_11039][aco_13045][aco_15170]<|reserved_special_token_88|>[aco_3507][aco_5327][aco_7626][aco_9786][aco_10921][aco_12323][aco_15492][aco_521][aco_3820][aco_5027][aco_7657][aco_8070][aco_10783][aco_12737][aco_15593][aco_249][aco_3140][aco_5661][aco_7549][aco_8226][aco_11545][aco_12342][aco_15209][aco_270][aco_3368][aco_5827][aco_7677][aco_8863][aco_10551][aco_13963][aco_15286][aco_890][aco_3270][aco_4952][aco_6633][aco_8284][aco_10353][aco_12625][aco_15704][aco_979][aco_2586][aco_5864][aco_6800][aco_7983][aco_11115][aco_13809][aco_15209][aco_725][aco_3658][aco_4663][aco_7884][aco_9275][aco_11414][aco_12124][aco_15093][aco_1452][aco_2046][aco_4278][aco_7842][aco_9018][aco_10498][aco_13794][aco_14856][aco_1217][aco_2615][aco_4804][aco_7828][aco_8968][aco_11544][aco_13729][aco_14468][aco_1102][aco_2161][aco_4163][aco_5945][aco_8776][aco_10953][aco_13992][aco_15932][aco_246][aco_2848][aco_5466][aco_6052][aco_8517][aco_11014][aco_13544][aco_16088][aco_1012][aco_3296][aco_3952][aco_6502][aco_9498][aco_10389][aco_12707][aco_15693][aco_629][aco_2475][aco_4786][aco_6420][aco_9811][aco_10433][aco_12897][aco_14262][aco_1302][aco_2306][aco_4506][aco_6891][aco_8223][aco_10249][aco_12898][aco_14698][aco_996][aco_2046][aco_5367][aco_7467][aco_8517][aco_10564][aco_13405][aco_14660][aco_1004][aco_3824][aco_4630][aco_7379][aco_9433][aco_11820][aco_13370][aco_14803][aco_48][aco_3779][aco_4498][aco_7707][aco_9132][aco_11478][aco_12074][aco_15335][aco_1375][aco_2658][aco_5407][aco_6019][aco_9468][aco_11688][aco_12851][aco_14648][aco_1309][aco_2230][aco_4466][aco_7898][aco_8473][aco_10291][aco_12784][aco_14231]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TTSTokenizer(text_tokenizer_name='tts_tokenizer', audio_tokenizer_name='tts_tokenizer')\n",
    "audio_decoding = tokenizer.decode(tokens=torch.tensor(weave_audio))\n",
    "text_decoding = tokenizer.decode(tokens=torch.tensor([128000, 39628, 11, 304, 279, 1193, 5647, 449, 902, 584, 527, 520, 3118, 11920, 11, 44642, 505, 1455, 422, 539, 505, 682, 279, 19071, 323, 44948, 15609, 304, 279, 68033]))\n",
    "print(text_decoding)\n",
    "print(audio_decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appending the tokens to a single sequence\n",
    "#text_tokens, task_tokens, speaker_tokens, audio_start_tokens, audio_tokens, common_stop_token.\n",
    "tokenizer = TTSTokenizer(text_tokenizer_name='tts_tokenizer', audio_tokenizer_name='tts_tokenizer')\n",
    "\n",
    "MIMI = '[mimi]'\n",
    "CONVERT = '[convert]'\n",
    "CONTINUE = '[continue]'\n",
    "DEFAULT_SPEAKER = '[spkr_unk]'\n",
    "COMMON_STOP = '[stop]'\n",
    "\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def append_tokens(text, audio_tokens, speaker=DEFAULT_SPEAKER):\n",
    "    audio_tokens = torch.tensor(audio_tokens, dtype=torch.int32).clone().detach()\n",
    "    text_tokens = torch.tensor(tokenizer.encode(text), dtype=torch.int32).view(-1).clone().detach()\n",
    "    convert_tokens = torch.tensor(tokenizer.encode(CONVERT, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
    "    continue_tokens = torch.tensor(tokenizer.encode(CONTINUE, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
    "    speaker_tokens = torch.tensor(tokenizer.encode(speaker, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
    "    mimi_tokens = torch.tensor(tokenizer.encode(MIMI, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
    "    stop_tokens = torch.tensor(tokenizer.encode(COMMON_STOP, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
    "    \n",
    "    result = torch.cat([\n",
    "        text_tokens,\n",
    "        convert_tokens,\n",
    "        #continue_tokens,\n",
    "        speaker_tokens,\n",
    "        mimi_tokens,\n",
    "        audio_tokens,\n",
    "        stop_tokens\n",
    "    ])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115641/1348805275.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  text_tokens = torch.tensor(tokenizer.encode(text), dtype=torch.int32).view(-1).clone().detach()\n",
      "/tmp/ipykernel_115641/1348805275.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  convert_tokens = torch.tensor(tokenizer.encode(CONVERT, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
      "/tmp/ipykernel_115641/1348805275.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  continue_tokens = torch.tensor(tokenizer.encode(CONTINUE, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
      "/tmp/ipykernel_115641/1348805275.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  speaker_tokens = torch.tensor(tokenizer.encode(speaker, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
      "/tmp/ipykernel_115641/1348805275.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mimi_tokens = torch.tensor(tokenizer.encode(MIMI, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
      "/tmp/ipykernel_115641/1348805275.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  stop_tokens = torch.tensor(tokenizer.encode(COMMON_STOP, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n"
     ]
    }
   ],
   "source": [
    "result = append_tokens(\"Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition\", weave_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000,  39628,     11,    304,    279,   1193,   5647,    449,    902,\n",
       "           584,    527,    520,   3118,  11920,     11,  44642,    505,   1455,\n",
       "           422,    539,    505,    682,    279,  19071,    323,  44948,  15609,\n",
       "           304,    279,  68033, 144642, 144645, 144641, 128995, 130325, 132834,\n",
       "        135350, 138081, 138819, 140988, 143449, 128572, 131272, 133038, 135276,\n",
       "        136552, 139416, 141865, 142522, 129117, 131597, 133406, 134146, 136322,\n",
       "        139289, 141923, 143985, 129069, 131186, 133662, 135220, 136512, 139853,\n",
       "        141760, 142410, 129620, 131843, 133121, 134814, 137436, 138387, 141084,\n",
       "        142444, 128080, 131002, 133038, 135147, 137077, 139665, 141897, 142752,\n",
       "        129134, 131203, 132924, 134668, 136548, 140002, 140409, 143451, 128715,\n",
       "        130220, 133849, 135225, 136357, 139775, 140460, 143596, 129650, 131937,\n",
       "        132126, 134545, 136933, 139077, 141937, 143864, 129532, 131639, 132479,\n",
       "        134835, 137883, 138246, 141974, 143201, 128937, 131824, 133411, 134384,\n",
       "        136248, 138246, 141565, 143949, 129077, 130323, 132375, 134147, 138069,\n",
       "        139010, 141512, 142954, 129352, 130386, 133860, 134608, 138148, 140177,\n",
       "        141688, 143239, 128749, 130955, 133781, 135938, 137985, 140133, 141270,\n",
       "        143942, 128577, 130291, 133245, 134258, 137928, 138587, 141264, 142904,\n",
       "        128670, 131159, 132503, 135148, 136914, 139760, 140828, 142475, 129701,\n",
       "        130488, 133298, 134413, 136709, 138498, 140735, 142696, 129762, 131203,\n",
       "        132992, 136027, 136402, 138246, 141713, 142997, 128570, 131397, 133382,\n",
       "        134994, 136195, 139013, 142075, 142487, 129468, 130744, 133054, 135043,\n",
       "        136663, 140173, 141663, 142607, 128157, 132044, 132786, 135857, 136657,\n",
       "        138778, 140733, 142574, 128839, 131041, 133885, 134696, 136357, 139132,\n",
       "        140508, 144299, 129457, 131094, 132982, 135497, 137717, 140221, 141218,\n",
       "        142767, 128181, 131341, 133840, 135517, 136861, 140137, 141400, 142794,\n",
       "        129022, 131672, 132922, 135561, 137960, 138869, 140476, 143155, 129037,\n",
       "        130437, 132452, 134366, 136541, 138498, 140552, 143492, 129067, 131891,\n",
       "        132160, 135811, 137641, 138246, 140940, 143208, 129132, 130387, 133891,\n",
       "        136062, 137641, 139273, 141345, 143673, 129320, 131104, 133274, 134286,\n",
       "        137706, 138808, 141526, 143887, 129607, 130291, 133245, 134258, 137207,\n",
       "        138795, 142159, 143942, 129607, 131259, 133245, 134434, 136773, 139812,\n",
       "        141797, 143942, 128663, 130291, 133655, 135492, 137107, 139683, 141526,\n",
       "        144344, 129779, 130291, 133655, 135492, 136773, 139683, 142159, 144344,\n",
       "        129301, 130291, 133655, 135492, 136459, 139683, 141526, 144080, 129618,\n",
       "        131765, 133686, 135920, 136515, 138568, 140382, 142654, 129782, 130680,\n",
       "        134088, 135388, 138099, 139164, 141007, 142836, 128304, 130689, 133573,\n",
       "        135245, 138128, 140000, 141169, 144180, 129219, 131390, 134066, 134549,\n",
       "        136840, 139839, 142026, 143399, 129640, 130582, 133627, 134822, 136510,\n",
       "        138754, 141771, 143770, 129196, 130787, 134070, 136099, 136409, 138550,\n",
       "        142227, 142524, 128505, 130783, 133420, 135114, 137860, 140104, 140948,\n",
       "        144171, 129146, 130063, 132779, 135688, 137530, 139481, 141406, 142520,\n",
       "        128784, 131671, 132450, 136045, 137666, 138890, 141700, 143080, 128917,\n",
       "        131754, 132133, 134842, 137151, 139775, 142253, 143311, 129708, 130779,\n",
       "        133640, 135447, 136465, 139175, 141048, 142408, 128922, 130386, 132886,\n",
       "        134893, 136343, 139580, 141355, 142594, 129137, 130821, 132533, 135072,\n",
       "        136226, 139759, 140997, 143102, 129358, 130868, 133712, 134398, 137393,\n",
       "        139796, 140534, 143363, 128126, 130538, 132762, 134984, 136773, 139979,\n",
       "        140954, 143887, 128832, 131259, 133793, 135492, 137928, 139809, 141526,\n",
       "        144080, 129743, 130291, 133655, 135492, 137928, 139809, 141113, 144344,\n",
       "        128663, 130291, 133245, 134308, 136773, 139270, 142159, 143942, 129544,\n",
       "        131104, 132879, 135492, 137928, 139812, 141113, 144344, 128835, 130941,\n",
       "        132602, 135049, 136666, 139613, 140860, 144269, 128337, 131187, 133089,\n",
       "        134987, 136678, 139449, 140465, 143899, 129377, 131817, 133781, 134413,\n",
       "        136337, 140221, 141484, 143470, 130024, 130741, 133627, 134670, 137965,\n",
       "        138514, 141771, 142828, 129252, 130240, 133686, 134497, 138092, 140015,\n",
       "        142160, 143434, 129260, 130871, 133216, 135834, 137580, 139135, 141863,\n",
       "        142524, 128304, 130417, 132762, 135214, 138040, 139256, 141175, 142728,\n",
       "        129039, 131825, 133669, 135608, 138197, 138794, 140372, 143522, 129640,\n",
       "        130696, 133360, 136125, 138013, 138913, 141370, 142998, 128797, 131259,\n",
       "        133598, 134286, 136459, 138587, 141526, 143749, 128262, 131259, 133655,\n",
       "        135492, 136532, 139809, 141526, 144080, 128663, 130291, 133655, 135492,\n",
       "        137928, 139809, 142266, 144344, 128384, 131397, 132762, 135841, 138197,\n",
       "        138794, 141281, 144204, 130025, 130837, 133686, 134666, 137959, 139039,\n",
       "        141433, 143179, 129774, 130979, 133778, 134741, 137010, 138806, 141633,\n",
       "        143066, 128178, 130680, 133776, 134877, 136409, 139577, 140681, 143858,\n",
       "        128129, 131251, 133595, 135833, 136396, 138246, 140669, 142598, 129669,\n",
       "        130333, 132352, 134878, 136717, 138246, 141089, 143782, 128421, 130505,\n",
       "        132348, 134838, 137561, 138741, 142305, 143603, 129519, 131958, 132468,\n",
       "        135590, 136966, 139421, 141476, 142660, 129022, 130951, 133706, 135882,\n",
       "        136521, 139121, 141580, 143632, 129562, 130376, 132313, 134835, 137641,\n",
       "        138246, 140703, 143165, 129252, 131627, 133411, 134736, 136208, 139572,\n",
       "        141000, 142947, 129782, 130468, 133482, 134796, 136859, 140087, 141757,\n",
       "        143059, 128304, 131566, 133203, 135172, 136604, 139309, 141337, 143786,\n",
       "        129882, 130079, 133471, 135332, 136465, 139874, 142303, 142508, 129721,\n",
       "        130225, 133411, 135207, 137798, 138246, 141517, 143754, 129356, 131988,\n",
       "        133442, 135049, 137706, 139295, 141301, 143426, 128096, 131763, 133583,\n",
       "        135882, 138042, 139177, 140579, 143748, 128777, 132076, 133283, 135913,\n",
       "        136326, 139039, 140993, 143849, 128505, 131396, 133917, 135805, 136482,\n",
       "        139801, 140598, 143465, 128526, 131624, 134083, 135933, 137119, 138807,\n",
       "        142219, 143542, 129146, 131526, 133208, 134889, 136540, 138609, 140881,\n",
       "        143960, 129235, 130842, 134120, 135056, 136239, 139371, 142065, 143465,\n",
       "        128981, 131914, 132919, 136140, 137531, 139670, 140380, 143349, 129708,\n",
       "        130302, 132534, 136098, 137274, 138754, 142050, 143112, 129473, 130871,\n",
       "        133060, 136084, 137224, 139800, 141985, 142724, 129358, 130417, 132419,\n",
       "        134201, 137032, 139209, 142248, 144188, 128502, 131104, 133722, 134308,\n",
       "        136773, 139270, 141800, 144344, 129268, 131552, 132208, 134758, 137754,\n",
       "        138645, 140963, 143949, 128885, 130731, 133042, 134676, 138067, 138689,\n",
       "        141153, 142518, 129558, 130562, 132762, 135147, 136479, 138505, 141154,\n",
       "        142954, 129252, 130302, 133623, 135723, 136773, 138820, 141661, 142916,\n",
       "        129260, 132080, 132886, 135635, 137689, 140076, 141626, 143059, 128304,\n",
       "        132035, 132754, 135963, 137388, 139734, 140330, 143591, 129631, 130914,\n",
       "        133663, 134275, 137724, 139944, 141107, 142904, 129565, 130486, 132722,\n",
       "        136154, 136729, 138547, 141040, 142487, 144644], dtype=torch.int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "CACHE_DIR = '/home/subhash/.cache/indri'\n",
    "def load_tokens(dataset_dir):\n",
    "    metadata_path = f\"{CACHE_DIR}/{dataset_dir}/annotation/metadata.jsonl\"\n",
    "    tokens_dir = os.path.join(CACHE_DIR, dataset_dir, 'tokens', 'mimi')\n",
    "    with open(metadata_path, 'r', encoding='utf-8') as file:\n",
    "        for line_number, line in enumerate(file, 1):\n",
    "            data = json.loads(line.strip())            \n",
    "            file_path = os.path.join(tokens_dir, data['id'] + '.npy')\n",
    "            \n",
    "            audio_tokens = np.load(file_path)\n",
    "            weave_audio = weave_tokens(audio_tokens.tolist())\n",
    "            yield data['raw_text'], weave_audio, data['speaker_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([128000,  39628,     11,  ..., 140557, 142493, 144644],\n",
      "       dtype=torch.int32)\n",
      "<|begin_of_text|>Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition[convert][spkr_unk][mimi][aco_1442][aco_3215][aco_4335][aco_6316][aco_8867][aco_11749][aco_14000][aco_15936][aco_1463][aco_3102][aco_4741][aco_6853][aco_9438][aco_11023][aco_12637][aco_15755][aco_12][aco_3327][aco_5098][aco_7331][aco_8508][aco_11952][aco_12220][aco_14360][aco_1133][aco_3558][aco_5358][aco_6739][aco_7940][aco_11184][aco_12999][aco_15792][aco_595][aco_2796][aco_5106][aco_7093][aco_8529][aco_11455][aco_13701][aco_15488][aco_1516][aco_2061][aco_5212][aco_6954][aco_8614][aco_10897][aco_13674][aco_14271]<|reserved_special_token_178|>[aco_2240][aco_4828][aco_6706][aco_8849][aco_10478][aco_12378][aco_15799][aco_1051][aco_3045][aco_4302][aco_7390][aco_8951][aco_11910][aco_12257][aco_15170][aco_1639][aco_2699][aco_4302][aco_6019][aco_8955][aco_11823][aco_12501][aco_15948][aco_1699][aco_2848][aco_5018][aco_7236][aco_8276][aco_11556][aco_12851][aco_14252][aco_1377][aco_2945][aco_4099][aco_7857][aco_8632][aco_10208][aco_12454][aco_15997][aco_515][aco_2083][aco_5832][aco_6442][aco_9266][aco_11856][aco_14011][aco_15956][aco_670][aco_2425][aco_5215][aco_6916][aco_9787][aco_10366][aco_12853][aco_14841][aco_1274][aco_1994][aco_4686][aco_6711][aco_9467][aco_10276][aco_12540][aco_15436][aco_1733][aco_2759][aco_5271][aco_6301][aco_9913][aco_11306][aco_13253][aco_14183][aco_1318][aco_3416][aco_4927][aco_6776][aco_9026][aco_10721][aco_12717][aco_16054][aco_1318][aco_3631][aco_5565][aco_7158][aco_9050][aco_10001][aco_12212][aco_15585][aco_466][aco_1914][aco_5193][aco_6968][aco_9149][aco_10985][aco_13654][aco_14271]<|reserved_special_token_136|>[aco_3601][aco_4382][aco_6809][aco_8835][aco_10527][aco_14059][aco_14945][aco_1689][aco_2947][aco_4347][aco_6079][aco_8821][aco_9990][aco_13675][aco_15317][aco_22][aco_2318][aco_4112][aco_7713][aco_9016][aco_10310][aco_13480][aco_15476][aco_853][aco_1951][aco_5839][aco_7063][aco_8790][aco_10921][aco_13748][aco_14291]<|reserved_special_token_21|>[aco_3315][aco_5851][aco_7695][aco_8724][aco_10321][aco_12909][aco_15491][aco_355][aco_3766][aco_4770][aco_7898][aco_9385][aco_9990][aco_12572][aco_15424]<|reserved_special_token_38|>[aco_2721][aco_5163][aco_6945][aco_9225][aco_11005][aco_12572][aco_14779][aco_366][aco_2205][aco_4426][aco_7872][aco_9048][aco_10332][aco_12926][aco_16039][aco_372][aco_3026][aco_5034][aco_6531][aco_9476][aco_10537][aco_12738][aco_15306][aco_1484][aco_3750][aco_4225][aco_6715][aco_9374][aco_12002][aco_12653][aco_16074][aco_316][aco_3460][aco_5374][aco_7713][aco_8527][aco_10657][aco_12682][aco_14713][aco_316][aco_2713][aco_5745][aco_7839][aco_9385][aco_10235][aco_13583][aco_15341][aco_89][aco_2343][aco_5832][aco_6930][aco_8026][aco_11198][aco_12297][aco_15611][aco_1733][aco_2949][aco_3909][aco_6296][aco_9795][aco_10079][aco_13518][aco_14740][aco_673][aco_3253][aco_5577][aco_6211][aco_8627][aco_10317][aco_12263][aco_14160][aco_673][aco_3839][aco_4299][aco_5905][aco_9520][aco_10122][aco_12466][aco_14094][aco_1520][aco_2202][aco_3975][aco_7713][aco_9526][aco_10908][aco_12320][aco_16062][aco_493][aco_2046][aco_5580][aco_6498][aco_9247][aco_11551][aco_13743][aco_15837][aco_57][aco_2575][aco_4032][aco_6356][aco_9488][aco_11312][aco_12116][aco_15229][aco_1741][aco_3202][aco_5839][aco_6479][aco_9071][aco_11243][aco_12259][aco_14104][aco_1315][aco_2995][aco_4558][aco_5990][aco_8020][aco_11781][aco_13532][aco_15187][aco_563][aco_2506][aco_5064][aco_6369][aco_8600][aco_11703][aco_12843][aco_14429][aco_982][aco_2713][aco_4812][aco_7805][aco_8340][aco_10641][aco_13755][aco_14945][aco_798][aco_2213][aco_5170][aco_7162][aco_8310][aco_11876][aco_12466][aco_15670][aco_798][aco_3315][aco_4561][aco_5917][aco_8654][aco_11544][aco_14024][aco_14600][aco_879][aco_3278][aco_5884][aco_6223][aco_8821][aco_11017][aco_13391][aco_15195][aco_856][aco_3496][aco_4589][aco_7332][aco_9430][aco_11433][aco_13435][aco_14945][aco_1137][aco_3639][aco_4952][aco_5986][aco_8351][aco_11134][aco_12470][aco_14306][aco_360][aco_3824][aco_4944][aco_6163][aco_8353][aco_10189][aco_13886][aco_15128][aco_1446][aco_2803][aco_5139][aco_7218][aco_8147][aco_11126][aco_13845][aco_16078][aco_737][aco_3315][aco_4509][aco_7241][aco_8145][aco_11458][aco_13811][aco_15837][aco_1291][aco_3338][aco_4066][aco_7852][aco_7978][aco_10785][aco_13823][aco_15837][aco_230][aco_3003][aco_4989][aco_7236][aco_8759][aco_10284][aco_13286][aco_15519][aco_230][aco_2848][aco_5018][aco_6632][aco_8851][aco_10038][aco_13008][aco_15260][aco_1783][aco_2035][aco_5018][aco_7236][aco_8517][aco_10539][aco_12358][aco_15353][aco_1712][aco_2035][aco_4989][aco_7236][aco_8417][aco_11014][aco_14010][aco_15824][aco_401][aco_2035][aco_5399][aco_6019][aco_9672][aco_11014][aco_13008][aco_15948][aco_408][aco_3303][aco_4731][aco_7471][aco_8483][aco_10884][aco_12711][aco_15587][aco_83][aco_2042][aco_5250][aco_6465][aco_8619][aco_10924][aco_13004][aco_14842][aco_83][aco_2682][aco_5742][aco_7104][aco_9771][aco_12025][aco_13535][aco_15810][aco_332][aco_3534][aco_4992][aco_7230][aco_8904][aco_10578][aco_12288][aco_15123][aco_1753][aco_1849][aco_5194][aco_6111][aco_8918][aco_10102][aco_13719][aco_14342][aco_654][aco_3383][aco_4437][aco_7024][aco_8576][aco_11997][aco_13035][aco_14945]<|reserved_special_token_24|>[aco_2549][aco_4878][aco_6734][aco_8239][aco_11896][aco_12301][aco_14945][aco_1064][aco_2842][aco_5155][aco_6436][aco_8417][aco_11918][aco_12192][aco_14093]<|reserved_special_token_139|>[aco_3430][aco_4987][aco_6241][aco_9864][aco_11024][aco_13968][aco_15143][aco_184][aco_2811][aco_5698][aco_6688][aco_9684][aco_11843][aco_12854][aco_14271]<|eom_id|>[aco_3631][aco_4107][aco_6122][aco_8653][aco_11566][aco_12398][aco_15967][aco_1466][aco_3473][aco_4592][aco_7668][aco_9477][aco_11859][aco_13053][aco_14108][aco_1526][aco_2930][aco_4091][aco_6586][aco_8181][aco_11660][aco_13068][aco_14279][aco_1394][aco_2353][aco_4358][aco_6399][aco_9061][aco_10868][aco_13007][aco_15058][aco_1359][aco_3278][aco_5583][aco_6042][aco_9858][aco_11746][aco_12110][aco_14945][aco_542][aco_2094][aco_5189][aco_7084][aco_7992][aco_10042][aco_12948][aco_15348][aco_661][aco_3023][aco_5374][aco_6773][aco_9082][aco_11543][aco_12886][aco_14191][aco_47][aco_3568][aco_5430][aco_7149][aco_9051][aco_11552][aco_13648][aco_15017][aco_441][aco_3331][aco_4230][aco_6335][aco_9202][aco_11501][aco_12729][aco_14487][aco_581][aco_2930][aco_4823][aco_7690][aco_8401][aco_10378][aco_13588][aco_14601][aco_102][aco_2341][aco_5815][aco_7649][aco_9050][aco_11744][aco_12869][aco_15667][aco_1626][aco_2682][aco_4001][aco_7852][aco_8894][aco_11319][aco_12925][aco_14648][aco_184][aco_2811][aco_4000][aco_6885][aco_8741][aco_11170][aco_12072][aco_15423][aco_1736][aco_3140][aco_5371][aco_6213][aco_8221][aco_11589][aco_12942][aco_14933][aco_1736][aco_2046][aco_4163][aco_6784][aco_9253][aco_11319][aco_13099][aco_15945][aco_331][aco_2723][aco_5691][aco_7770][aco_9763][aco_10858][aco_13589][aco_15467]<|reserved_special_token_170|>[aco_2684][aco_4050][aco_6679][aco_9574][aco_10562][aco_13435][aco_15338]<|reserved_special_token_170|>[aco_2811][aco_4541][aco_6549][aco_9616][aco_11505][aco_12654][aco_15285][aco_1371][aco_1848][aco_5055][aco_7711][aco_8445][aco_11779][aco_13759][aco_15850][aco_1274][aco_2796][aco_3966][aco_7200][aco_9543][aco_11809][aco_13984][aco_15732][aco_673][aco_2143][aco_4200][aco_6679][aco_9674][aco_11809][aco_12103][aco_15621][aco_1354][aco_3038][aco_4952][aco_6637][aco_8336][aco_11970][aco_12861][aco_15438][aco_1660][aco_2739][aco_5535][aco_6188][aco_9734][aco_10465][aco_12641][aco_15890][aco_267][aco_3592][aco_4523][aco_6579][aco_9575][aco_9990][aco_13343][aco_15409]<|reserved_special_token_205|>[aco_2676][aco_4192][aco_6579][aco_8775][aco_11656][aco_13563][aco_14103][aco_996][aco_3141][aco_5770][aco_7087][aco_8846][aco_10941][aco_12769][aco_14988][aco_1224][aco_3297][aco_5590][aco_7567][aco_9743][aco_10427][aco_12969][aco_15509][aco_1212][aco_2774][aco_4287][aco_7093][aco_9384][aco_10455][aco_13245][aco_14602][aco_1643][aco_2774][aco_5279][aco_7028][aco_8437][aco_11420][aco_12994][aco_15833][aco_295][aco_3430][aco_4001][aco_7806][aco_9802][aco_11017][aco_12703][aco_14231][aco_1777][aco_3230][aco_5764][aco_7824][aco_9001][aco_11828][aco_13796][aco_15143][aco_1777][aco_3322][aco_4789][aco_7771][aco_8642][aco_9990][aco_13187][aco_15502]<|reserved_special_token_75|>[aco_2516][aco_4738][aco_6234][aco_7976][aco_10498][aco_13933][aco_15037][aco_890][aco_3473][aco_3874][aco_7386][aco_9054][aco_10304][aco_12378][aco_15730][aco_528][aco_2607][aco_5785][aco_7534][aco_9160][aco_11028][aco_12339][aco_15049][aco_1039][aco_3020][aco_3887][aco_6526][aco_8392][aco_11200][aco_13442][aco_15635]<|reserved_special_token_191|>[aco_3216][aco_5020][aco_6450][aco_9101][aco_11464][aco_12041][aco_14653][aco_853][aco_3605][aco_5839][aco_5891][aco_9959][aco_10915][aco_12189][aco_15975][aco_12][aco_3460][aco_4070][aco_6951][aco_8837][aco_10284][aco_13642][aco_15253]<|start_header_id|>[aco_3407][aco_4602][aco_6182][aco_8805][aco_11153][aco_12059][aco_15019]<|start_header_id|>[aco_2578][aco_5585][aco_7152][aco_8514][aco_10207][aco_13958][aco_14861][aco_1525][aco_3311][aco_3996][aco_7149][aco_9699][aco_12030][aco_12869][aco_15253][aco_1223][aco_2482][aco_4212][aco_7425][aco_8896][aco_10375][aco_13379][aco_15492][aco_1274][aco_3507][aco_4609][aco_7433][aco_8224][aco_11222][aco_12911][aco_15002][aco_1274][aco_3407][aco_4522][aco_6351][aco_9155][aco_11254][aco_13487][aco_14857]<|reserved_special_token_138|>[aco_3062][aco_5484][aco_7402][aco_8907][aco_10591][aco_12755][aco_15804][aco_1782][aco_2656][aco_4558][aco_6758][aco_8215][aco_10614][aco_13583][aco_15284][aco_728][aco_2947][aco_4662][aco_6067][aco_9064][aco_10410][aco_13201][aco_15195][aco_1147][aco_3068][aco_4676][aco_6436][aco_9078][aco_10112][aco_12566][aco_14750][aco_350][aco_3827][aco_5203][aco_6085][aco_8682][aco_11439][aco_12160][aco_15186][aco_1123][aco_3023][aco_5864][aco_7428][aco_7999][aco_11548][aco_12510][aco_14750][aco_1584][aco_3322][aco_5413][aco_6061][aco_8929][aco_10591][aco_12442][aco_14945][aco_916][aco_2631][aco_4435][aco_7051][aco_8050][aco_10663][aco_12420][aco_15750][aco_1424][aco_2240][aco_5568][aco_7742][aco_9826][aco_11781][aco_12594][aco_15347][aco_906][aco_2699][aco_5466][aco_6609][aco_7980][aco_11851][aco_13366][aco_14382][aco_980][aco_2230][aco_4466][aco_7052][aco_9552][aco_11182][aco_12301][aco_14237][stop]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115641/1348805275.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  text_tokens = torch.tensor(tokenizer.encode(text), dtype=torch.int32).view(-1).clone().detach()\n",
      "/tmp/ipykernel_115641/1348805275.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  convert_tokens = torch.tensor(tokenizer.encode(CONVERT, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
      "/tmp/ipykernel_115641/1348805275.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  continue_tokens = torch.tensor(tokenizer.encode(CONTINUE, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
      "/tmp/ipykernel_115641/1348805275.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  speaker_tokens = torch.tensor(tokenizer.encode(speaker, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
      "/tmp/ipykernel_115641/1348805275.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mimi_tokens = torch.tensor(tokenizer.encode(MIMI, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
      "/tmp/ipykernel_115641/1348805275.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  stop_tokens = torch.tensor(tokenizer.encode(COMMON_STOP, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n"
     ]
    }
   ],
   "source": [
    "dataset = 'lj_speech'\n",
    "for raw_text, audio_tokens, speaker in load_tokens(dataset_dir=dataset):\n",
    "    with open('allowed_speakers.jsonl', 'r', encoding='utf-8') as file:\n",
    "        allowed_speakers = [json.loads(line.strip()) for line in file]\n",
    "    entry = next((item for item in allowed_speakers if item['dataset'] == dataset and item['speaker'] == speaker), None)\n",
    "    if entry:\n",
    "        combined = entry['combined']\n",
    "    else:\n",
    "        combined = DEFAULT_SPEAKER\n",
    "    result = append_tokens(raw_text, audio_tokens, speaker=combined)\n",
    "    print(result)\n",
    "    print(tokenizer.decode(result))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (replace this with your actual data)\n",
    "\n",
    "\n",
    "# Function to get combined ID based on dataset and speaker\n",
    "def get_combined_id(data, dataset, speaker):\n",
    "    entry = next((item for item in data if item['dataset'] == dataset and item['speaker'] == speaker), None)\n",
    "    return entry['combined'] if entry else None\n",
    "\n",
    "# Example usage\n",
    "dataset_input = \"mls_eng_10k\"\n",
    "speaker_input = \"2156\"\n",
    "combined_id = get_combined_id(data, dataset_input, speaker_input)\n",
    "\n",
    "print(combined_id)  # Output: [spkr_mls_eng_10k_2156]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subhash/miniconda3/lib/python3.12/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13100\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def load_pickle_file(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "pickle_file_path = 'tokens/lj_speech_tokens.pkl'  \n",
    "data = load_pickle_file(pickle_file_path)\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
