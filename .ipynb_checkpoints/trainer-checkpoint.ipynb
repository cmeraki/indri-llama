{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 21:17:57.807816: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-17 21:17:57.815273: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734450477.823825 2237071 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734450477.826167 2237071 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-17 21:17:57.835489: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Llama Tokenizer - Step 1\n",
    "from transformers import MimiModel, AutoFeatureExtractor, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TextTokenizer:\n",
    "    def __init__(self, name='Llama_tokenizer'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(name, legacy=False)\n",
    "        print(\"text vocab size\", self.tokenizer.vocab_size)\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return self.tokenizer.decode(tokens)\n",
    "    \n",
    "class MimiTokenizer:\n",
    "    def __init__(self, device):    \n",
    "        self.device = device\n",
    "        self.model = MimiModel.from_pretrained(\"kyutai/mimi\")\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(\"kyutai/mimi\", device=device)\n",
    "        self.sampling_rate = self.feature_extractor.sampling_rate\n",
    "        self.n_codebooks = 8\n",
    "        self.vocab_size = 2048\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def encode(self, waveform):\n",
    "        inputs = self.feature_extractor(raw_audio=waveform, \n",
    "                                        sampling_rate=self.sampling_rate, \n",
    "                                        return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "        output = self.model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"], num_quantizers=self.n_codebooks)\n",
    "        tokens = output.audio_codes[0].cpu().numpy()\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        assert len(tokens.shape) == 2\n",
    "        tokens = torch.tensor(np.expand_dims(tokens, axis=0)).to(self.device)\n",
    "        output = self.model.decode(tokens)\n",
    "        waveform = output.audio_values.cpu()\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text vocab size 128000\n",
      "Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition\n",
      "[128000, 39628, 11, 304, 279, 1193, 5647, 449, 902, 584, 527, 520, 3118, 11920, 11, 44642, 505, 1455, 422, 539, 505, 682, 279, 19071, 323, 44948, 15609, 304, 279, 68033]\n",
      "<|begin_of_text|>Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "tokenizer = TextTokenizer()\n",
    "with open('/home/subhash/.cache/indri/lj_speech/annotation/metadata.jsonl') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        text = data['raw_text']\n",
    "        tokens = tokenizer.encode(text)\n",
    "        print(text)\n",
    "        print(tokens)\n",
    "        print(tokenizer.decode(tokens))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mimi Tokeniser - Step 2\n",
    "import numpy as np\n",
    "mimi_tokens = np.load(\"/home/subhash/.cache/indri/lj_speech/tokens/mimi/LJ040-0046.npy\")\n",
    "print(mimi_tokens.shape)\n",
    "#print(mimi_tokens)\n",
    "type(mimi_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([128995, 130325, 132834, 135350, 138081, 138819, 140988, 143449,\n",
       "       128572, 131272, 133038, 135276, 136552, 139416, 141865, 142522,\n",
       "       129117, 131597, 133406, 134146, 136322, 139289, 141923, 143985,\n",
       "       129069, 131186, 133662, 135220, 136512, 139853, 141760, 142410,\n",
       "       129620, 131843, 133121, 134814, 137436, 138387, 141084, 142444,\n",
       "       128080, 131002, 133038, 135147, 137077, 139665, 141897, 142752,\n",
       "       129134, 131203, 132924, 134668, 136548, 140002, 140409, 143451,\n",
       "       128715, 130220, 133849, 135225, 136357, 139775, 140460, 143596,\n",
       "       129650, 131937, 132126, 134545, 136933, 139077, 141937, 143864,\n",
       "       129532, 131639, 132479, 134835, 137883, 138246, 141974, 143201,\n",
       "       128937, 131824, 133411, 134384, 136248, 138246, 141565, 143949,\n",
       "       129077, 130323, 132375, 134147, 138069, 139010, 141512, 142954,\n",
       "       129352, 130386, 133860, 134608, 138148, 140177, 141688, 143239,\n",
       "       128749, 130955, 133781, 135938, 137985, 140133, 141270, 143942,\n",
       "       128577, 130291, 133245, 134258, 137928, 138587, 141264, 142904,\n",
       "       128670, 131159, 132503, 135148, 136914, 139760, 140828, 142475,\n",
       "       129701, 130488, 133298, 134413, 136709, 138498, 140735, 142696,\n",
       "       129762, 131203, 132992, 136027, 136402, 138246, 141713, 142997,\n",
       "       128570, 131397, 133382, 134994, 136195, 139013, 142075, 142487,\n",
       "       129468, 130744, 133054, 135043, 136663, 140173, 141663, 142607,\n",
       "       128157, 132044, 132786, 135857, 136657, 138778, 140733, 142574,\n",
       "       128839, 131041, 133885, 134696, 136357, 139132, 140508, 144299,\n",
       "       129457, 131094, 132982, 135497, 137717, 140221, 141218, 142767,\n",
       "       128181, 131341, 133840, 135517, 136861, 140137, 141400, 142794,\n",
       "       129022, 131672, 132922, 135561, 137960, 138869, 140476, 143155,\n",
       "       129037, 130437, 132452, 134366, 136541, 138498, 140552, 143492,\n",
       "       129067, 131891, 132160, 135811, 137641, 138246, 140940, 143208,\n",
       "       129132, 130387, 133891, 136062, 137641, 139273, 141345, 143673,\n",
       "       129320, 131104, 133274, 134286, 137706, 138808, 141526, 143887,\n",
       "       129607, 130291, 133245, 134258, 137207, 138795, 142159, 143942,\n",
       "       129607, 131259, 133245, 134434, 136773, 139812, 141797, 143942,\n",
       "       128663, 130291, 133655, 135492, 137107, 139683, 141526, 144344,\n",
       "       129779, 130291, 133655, 135492, 136773, 139683, 142159, 144344,\n",
       "       129301, 130291, 133655, 135492, 136459, 139683, 141526, 144080,\n",
       "       129618, 131765, 133686, 135920, 136515, 138568, 140382, 142654,\n",
       "       129782, 130680, 134088, 135388, 138099, 139164, 141007, 142836,\n",
       "       128304, 130689, 133573, 135245, 138128, 140000, 141169, 144180,\n",
       "       129219, 131390, 134066, 134549, 136840, 139839, 142026, 143399,\n",
       "       129640, 130582, 133627, 134822, 136510, 138754, 141771, 143770,\n",
       "       129196, 130787, 134070, 136099, 136409, 138550, 142227, 142524,\n",
       "       128505, 130783, 133420, 135114, 137860, 140104, 140948, 144171,\n",
       "       129146, 130063, 132779, 135688, 137530, 139481, 141406, 142520,\n",
       "       128784, 131671, 132450, 136045, 137666, 138890, 141700, 143080,\n",
       "       128917, 131754, 132133, 134842, 137151, 139775, 142253, 143311,\n",
       "       129708, 130779, 133640, 135447, 136465, 139175, 141048, 142408,\n",
       "       128922, 130386, 132886, 134893, 136343, 139580, 141355, 142594,\n",
       "       129137, 130821, 132533, 135072, 136226, 139759, 140997, 143102,\n",
       "       129358, 130868, 133712, 134398, 137393, 139796, 140534, 143363,\n",
       "       128126, 130538, 132762, 134984, 136773, 139979, 140954, 143887,\n",
       "       128832, 131259, 133793, 135492, 137928, 139809, 141526, 144080,\n",
       "       129743, 130291, 133655, 135492, 137928, 139809, 141113, 144344,\n",
       "       128663, 130291, 133245, 134308, 136773, 139270, 142159, 143942,\n",
       "       129544, 131104, 132879, 135492, 137928, 139812, 141113, 144344,\n",
       "       128835, 130941, 132602, 135049, 136666, 139613, 140860, 144269,\n",
       "       128337, 131187, 133089, 134987, 136678, 139449, 140465, 143899,\n",
       "       129377, 131817, 133781, 134413, 136337, 140221, 141484, 143470,\n",
       "       130024, 130741, 133627, 134670, 137965, 138514, 141771, 142828,\n",
       "       129252, 130240, 133686, 134497, 138092, 140015, 142160, 143434,\n",
       "       129260, 130871, 133216, 135834, 137580, 139135, 141863, 142524,\n",
       "       128304, 130417, 132762, 135214, 138040, 139256, 141175, 142728,\n",
       "       129039, 131825, 133669, 135608, 138197, 138794, 140372, 143522,\n",
       "       129640, 130696, 133360, 136125, 138013, 138913, 141370, 142998,\n",
       "       128797, 131259, 133598, 134286, 136459, 138587, 141526, 143749,\n",
       "       128262, 131259, 133655, 135492, 136532, 139809, 141526, 144080,\n",
       "       128663, 130291, 133655, 135492, 137928, 139809, 142266, 144344,\n",
       "       128384, 131397, 132762, 135841, 138197, 138794, 141281, 144204,\n",
       "       130025, 130837, 133686, 134666, 137959, 139039, 141433, 143179,\n",
       "       129774, 130979, 133778, 134741, 137010, 138806, 141633, 143066,\n",
       "       128178, 130680, 133776, 134877, 136409, 139577, 140681, 143858,\n",
       "       128129, 131251, 133595, 135833, 136396, 138246, 140669, 142598,\n",
       "       129669, 130333, 132352, 134878, 136717, 138246, 141089, 143782,\n",
       "       128421, 130505, 132348, 134838, 137561, 138741, 142305, 143603,\n",
       "       129519, 131958, 132468, 135590, 136966, 139421, 141476, 142660,\n",
       "       129022, 130951, 133706, 135882, 136521, 139121, 141580, 143632,\n",
       "       129562, 130376, 132313, 134835, 137641, 138246, 140703, 143165,\n",
       "       129252, 131627, 133411, 134736, 136208, 139572, 141000, 142947,\n",
       "       129782, 130468, 133482, 134796, 136859, 140087, 141757, 143059,\n",
       "       128304, 131566, 133203, 135172, 136604, 139309, 141337, 143786,\n",
       "       129882, 130079, 133471, 135332, 136465, 139874, 142303, 142508,\n",
       "       129721, 130225, 133411, 135207, 137798, 138246, 141517, 143754,\n",
       "       129356, 131988, 133442, 135049, 137706, 139295, 141301, 143426,\n",
       "       128096, 131763, 133583, 135882, 138042, 139177, 140579, 143748,\n",
       "       128777, 132076, 133283, 135913, 136326, 139039, 140993, 143849,\n",
       "       128505, 131396, 133917, 135805, 136482, 139801, 140598, 143465,\n",
       "       128526, 131624, 134083, 135933, 137119, 138807, 142219, 143542,\n",
       "       129146, 131526, 133208, 134889, 136540, 138609, 140881, 143960,\n",
       "       129235, 130842, 134120, 135056, 136239, 139371, 142065, 143465,\n",
       "       128981, 131914, 132919, 136140, 137531, 139670, 140380, 143349,\n",
       "       129708, 130302, 132534, 136098, 137274, 138754, 142050, 143112,\n",
       "       129473, 130871, 133060, 136084, 137224, 139800, 141985, 142724,\n",
       "       129358, 130417, 132419, 134201, 137032, 139209, 142248, 144188,\n",
       "       128502, 131104, 133722, 134308, 136773, 139270, 141800, 144344,\n",
       "       129268, 131552, 132208, 134758, 137754, 138645, 140963, 143949,\n",
       "       128885, 130731, 133042, 134676, 138067, 138689, 141153, 142518,\n",
       "       129558, 130562, 132762, 135147, 136479, 138505, 141154, 142954,\n",
       "       129252, 130302, 133623, 135723, 136773, 138820, 141661, 142916,\n",
       "       129260, 132080, 132886, 135635, 137689, 140076, 141626, 143059,\n",
       "       128304, 132035, 132754, 135963, 137388, 139734, 140330, 143591,\n",
       "       129631, 130914, 133663, 134275, 137724, 139944, 141107, 142904,\n",
       "       129565, 130486, 132722, 136154, 136729, 138547, 141040, 142487])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weave Tokens, codebook offset - Step 3\n",
    "def weave_tokens(tokens):\n",
    "    result = []\n",
    "    max_length = max(len(codebook) for codebook in tokens)\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        for codebook_index, codebook in enumerate(tokens):\n",
    "            if i < len(codebook):\n",
    "                offset = 2048 * codebook_index + 128000\n",
    "                result.append(codebook[i] + offset)                 \n",
    "    return np.array(result)\n",
    "\n",
    "weave_audio = weave_tokens(mimi_tokens.tolist())\n",
    "weave_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class TTSTokenizer:\n",
    "    def __init__(self, text_tokenizer_name='tts_tokenizer', audio_tokenizer_name='tts_tokenizer'):\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(text_tokenizer_name, legacy=False)\n",
    "        self.audio_tokenizer = AutoTokenizer.from_pretrained(audio_tokenizer_name, legacy=False)\n",
    "        print(\"text vocab size\", self.audio_tokenizer.vocab_size)\n",
    "\n",
    "    def encode(self, input_data, add_special_tokens=True):\n",
    "        if isinstance(input_data, str):\n",
    "            encoded_tokens = self.text_tokenizer.encode(\n",
    "                input_data, \n",
    "                return_tensors='pt', \n",
    "                add_special_tokens=add_special_tokens\n",
    "            )\n",
    "            return encoded_tokens\n",
    "        elif isinstance(input_data, list) and all(isinstance(item, str) for item in input_data):\n",
    "            encoded_tokens = self.audio_tokenizer.encode(\n",
    "                input_data, \n",
    "                return_tensors='pt', \n",
    "                add_special_tokens=add_special_tokens\n",
    "            )\n",
    "            return encoded_tokens\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a string or a list of strings\")\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        if not isinstance(tokens, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch tensor of tokens\")\n",
    "        \n",
    "        try:\n",
    "            decoded_text = self.text_tokenizer.decode(tokens)\n",
    "            return decoded_text\n",
    "        except:\n",
    "            try:\n",
    "                decoded_tokens = self.audio_tokenizer.decode(tokens)\n",
    "                return torch.tensor(decoded_tokens)\n",
    "            except:\n",
    "                raise ValueError(\"Unable to decode the provided tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text vocab size 128000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'weave_audio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m TTSTokenizer(text_tokenizer_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtts_tokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m, audio_tokenizer_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtts_tokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m audio_decoding \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(tokens\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(weave_audio))\n\u001b[1;32m      3\u001b[0m text_decoding \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(tokens\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m128000\u001b[39m, \u001b[38;5;241m39628\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m304\u001b[39m, \u001b[38;5;241m279\u001b[39m, \u001b[38;5;241m1193\u001b[39m, \u001b[38;5;241m5647\u001b[39m, \u001b[38;5;241m449\u001b[39m, \u001b[38;5;241m902\u001b[39m, \u001b[38;5;241m584\u001b[39m, \u001b[38;5;241m527\u001b[39m, \u001b[38;5;241m520\u001b[39m, \u001b[38;5;241m3118\u001b[39m, \u001b[38;5;241m11920\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m44642\u001b[39m, \u001b[38;5;241m505\u001b[39m, \u001b[38;5;241m1455\u001b[39m, \u001b[38;5;241m422\u001b[39m, \u001b[38;5;241m539\u001b[39m, \u001b[38;5;241m505\u001b[39m, \u001b[38;5;241m682\u001b[39m, \u001b[38;5;241m279\u001b[39m, \u001b[38;5;241m19071\u001b[39m, \u001b[38;5;241m323\u001b[39m, \u001b[38;5;241m44948\u001b[39m, \u001b[38;5;241m15609\u001b[39m, \u001b[38;5;241m304\u001b[39m, \u001b[38;5;241m279\u001b[39m, \u001b[38;5;241m68033\u001b[39m]))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(text_decoding)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weave_audio' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = TTSTokenizer(text_tokenizer_name='tts_tokenizer', audio_tokenizer_name='tts_tokenizer')\n",
    "audio_decoding = tokenizer.decode(tokens=torch.tensor(weave_audio))\n",
    "text_decoding = tokenizer.decode(tokens=torch.tensor([128000, 39628, 11, 304, 279, 1193, 5647, 449, 902, 584, 527, 520, 3118, 11920, 11, 44642, 505, 1455, 422, 539, 505, 682, 279, 19071, 323, 44948, 15609, 304, 279, 68033]))\n",
    "print(text_decoding)\n",
    "print(audio_decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appending the tokens to a single sequence\n",
    "#text_tokens, task_tokens, speaker_tokens, audio_start_tokens, audio_tokens, common_stop_token.\n",
    "tokenizer = TTSTokenizer(text_tokenizer_name='tts_tokenizer', audio_tokenizer_name='tts_tokenizer')\n",
    "\n",
    "MIMI = '[mimi]'\n",
    "CONVERT = '[convert]'\n",
    "CONTINUE = '[continue]'\n",
    "DEFAULT_SPEAKER = '[spkr_unk]'\n",
    "COMMON_STOP = '[stop]'\n",
    "\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def append_tokens(text, audio_tokens, speaker=DEFAULT_SPEAKER):\n",
    "    audio_tokens = torch.tensor(audio_tokens, dtype=torch.int32).clone().detach()\n",
    "    text_tokens = torch.tensor(tokenizer.encode(text), dtype=torch.int32).view(-1).clone().detach()\n",
    "    convert_tokens = torch.tensor(tokenizer.encode(CONVERT, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
    "    continue_tokens = torch.tensor(tokenizer.encode(CONTINUE, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
    "    speaker_tokens = torch.tensor(tokenizer.encode(speaker, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
    "    mimi_tokens = torch.tensor(tokenizer.encode(MIMI, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
    "    stop_tokens = torch.tensor(tokenizer.encode(COMMON_STOP, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
    "    \n",
    "    result = torch.cat([\n",
    "        text_tokens,\n",
    "        convert_tokens,\n",
    "        #continue_tokens,\n",
    "        speaker_tokens,\n",
    "        mimi_tokens,\n",
    "        audio_tokens,\n",
    "        stop_tokens\n",
    "    ])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115641/1348805275.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  text_tokens = torch.tensor(tokenizer.encode(text), dtype=torch.int32).view(-1).clone().detach()\n",
      "/tmp/ipykernel_115641/1348805275.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  convert_tokens = torch.tensor(tokenizer.encode(CONVERT, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
      "/tmp/ipykernel_115641/1348805275.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  continue_tokens = torch.tensor(tokenizer.encode(CONTINUE, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
      "/tmp/ipykernel_115641/1348805275.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  speaker_tokens = torch.tensor(tokenizer.encode(speaker, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
      "/tmp/ipykernel_115641/1348805275.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mimi_tokens = torch.tensor(tokenizer.encode(MIMI, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
      "/tmp/ipykernel_115641/1348805275.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  stop_tokens = torch.tensor(tokenizer.encode(COMMON_STOP, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n"
     ]
    }
   ],
   "source": [
    "result = append_tokens(\"Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition\", weave_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000,  39628,     11,    304,    279,   1193,   5647,    449,    902,\n",
       "           584,    527,    520,   3118,  11920,     11,  44642,    505,   1455,\n",
       "           422,    539,    505,    682,    279,  19071,    323,  44948,  15609,\n",
       "           304,    279,  68033, 144642, 144645, 144641, 128995, 130325, 132834,\n",
       "        135350, 138081, 138819, 140988, 143449, 128572, 131272, 133038, 135276,\n",
       "        136552, 139416, 141865, 142522, 129117, 131597, 133406, 134146, 136322,\n",
       "        139289, 141923, 143985, 129069, 131186, 133662, 135220, 136512, 139853,\n",
       "        141760, 142410, 129620, 131843, 133121, 134814, 137436, 138387, 141084,\n",
       "        142444, 128080, 131002, 133038, 135147, 137077, 139665, 141897, 142752,\n",
       "        129134, 131203, 132924, 134668, 136548, 140002, 140409, 143451, 128715,\n",
       "        130220, 133849, 135225, 136357, 139775, 140460, 143596, 129650, 131937,\n",
       "        132126, 134545, 136933, 139077, 141937, 143864, 129532, 131639, 132479,\n",
       "        134835, 137883, 138246, 141974, 143201, 128937, 131824, 133411, 134384,\n",
       "        136248, 138246, 141565, 143949, 129077, 130323, 132375, 134147, 138069,\n",
       "        139010, 141512, 142954, 129352, 130386, 133860, 134608, 138148, 140177,\n",
       "        141688, 143239, 128749, 130955, 133781, 135938, 137985, 140133, 141270,\n",
       "        143942, 128577, 130291, 133245, 134258, 137928, 138587, 141264, 142904,\n",
       "        128670, 131159, 132503, 135148, 136914, 139760, 140828, 142475, 129701,\n",
       "        130488, 133298, 134413, 136709, 138498, 140735, 142696, 129762, 131203,\n",
       "        132992, 136027, 136402, 138246, 141713, 142997, 128570, 131397, 133382,\n",
       "        134994, 136195, 139013, 142075, 142487, 129468, 130744, 133054, 135043,\n",
       "        136663, 140173, 141663, 142607, 128157, 132044, 132786, 135857, 136657,\n",
       "        138778, 140733, 142574, 128839, 131041, 133885, 134696, 136357, 139132,\n",
       "        140508, 144299, 129457, 131094, 132982, 135497, 137717, 140221, 141218,\n",
       "        142767, 128181, 131341, 133840, 135517, 136861, 140137, 141400, 142794,\n",
       "        129022, 131672, 132922, 135561, 137960, 138869, 140476, 143155, 129037,\n",
       "        130437, 132452, 134366, 136541, 138498, 140552, 143492, 129067, 131891,\n",
       "        132160, 135811, 137641, 138246, 140940, 143208, 129132, 130387, 133891,\n",
       "        136062, 137641, 139273, 141345, 143673, 129320, 131104, 133274, 134286,\n",
       "        137706, 138808, 141526, 143887, 129607, 130291, 133245, 134258, 137207,\n",
       "        138795, 142159, 143942, 129607, 131259, 133245, 134434, 136773, 139812,\n",
       "        141797, 143942, 128663, 130291, 133655, 135492, 137107, 139683, 141526,\n",
       "        144344, 129779, 130291, 133655, 135492, 136773, 139683, 142159, 144344,\n",
       "        129301, 130291, 133655, 135492, 136459, 139683, 141526, 144080, 129618,\n",
       "        131765, 133686, 135920, 136515, 138568, 140382, 142654, 129782, 130680,\n",
       "        134088, 135388, 138099, 139164, 141007, 142836, 128304, 130689, 133573,\n",
       "        135245, 138128, 140000, 141169, 144180, 129219, 131390, 134066, 134549,\n",
       "        136840, 139839, 142026, 143399, 129640, 130582, 133627, 134822, 136510,\n",
       "        138754, 141771, 143770, 129196, 130787, 134070, 136099, 136409, 138550,\n",
       "        142227, 142524, 128505, 130783, 133420, 135114, 137860, 140104, 140948,\n",
       "        144171, 129146, 130063, 132779, 135688, 137530, 139481, 141406, 142520,\n",
       "        128784, 131671, 132450, 136045, 137666, 138890, 141700, 143080, 128917,\n",
       "        131754, 132133, 134842, 137151, 139775, 142253, 143311, 129708, 130779,\n",
       "        133640, 135447, 136465, 139175, 141048, 142408, 128922, 130386, 132886,\n",
       "        134893, 136343, 139580, 141355, 142594, 129137, 130821, 132533, 135072,\n",
       "        136226, 139759, 140997, 143102, 129358, 130868, 133712, 134398, 137393,\n",
       "        139796, 140534, 143363, 128126, 130538, 132762, 134984, 136773, 139979,\n",
       "        140954, 143887, 128832, 131259, 133793, 135492, 137928, 139809, 141526,\n",
       "        144080, 129743, 130291, 133655, 135492, 137928, 139809, 141113, 144344,\n",
       "        128663, 130291, 133245, 134308, 136773, 139270, 142159, 143942, 129544,\n",
       "        131104, 132879, 135492, 137928, 139812, 141113, 144344, 128835, 130941,\n",
       "        132602, 135049, 136666, 139613, 140860, 144269, 128337, 131187, 133089,\n",
       "        134987, 136678, 139449, 140465, 143899, 129377, 131817, 133781, 134413,\n",
       "        136337, 140221, 141484, 143470, 130024, 130741, 133627, 134670, 137965,\n",
       "        138514, 141771, 142828, 129252, 130240, 133686, 134497, 138092, 140015,\n",
       "        142160, 143434, 129260, 130871, 133216, 135834, 137580, 139135, 141863,\n",
       "        142524, 128304, 130417, 132762, 135214, 138040, 139256, 141175, 142728,\n",
       "        129039, 131825, 133669, 135608, 138197, 138794, 140372, 143522, 129640,\n",
       "        130696, 133360, 136125, 138013, 138913, 141370, 142998, 128797, 131259,\n",
       "        133598, 134286, 136459, 138587, 141526, 143749, 128262, 131259, 133655,\n",
       "        135492, 136532, 139809, 141526, 144080, 128663, 130291, 133655, 135492,\n",
       "        137928, 139809, 142266, 144344, 128384, 131397, 132762, 135841, 138197,\n",
       "        138794, 141281, 144204, 130025, 130837, 133686, 134666, 137959, 139039,\n",
       "        141433, 143179, 129774, 130979, 133778, 134741, 137010, 138806, 141633,\n",
       "        143066, 128178, 130680, 133776, 134877, 136409, 139577, 140681, 143858,\n",
       "        128129, 131251, 133595, 135833, 136396, 138246, 140669, 142598, 129669,\n",
       "        130333, 132352, 134878, 136717, 138246, 141089, 143782, 128421, 130505,\n",
       "        132348, 134838, 137561, 138741, 142305, 143603, 129519, 131958, 132468,\n",
       "        135590, 136966, 139421, 141476, 142660, 129022, 130951, 133706, 135882,\n",
       "        136521, 139121, 141580, 143632, 129562, 130376, 132313, 134835, 137641,\n",
       "        138246, 140703, 143165, 129252, 131627, 133411, 134736, 136208, 139572,\n",
       "        141000, 142947, 129782, 130468, 133482, 134796, 136859, 140087, 141757,\n",
       "        143059, 128304, 131566, 133203, 135172, 136604, 139309, 141337, 143786,\n",
       "        129882, 130079, 133471, 135332, 136465, 139874, 142303, 142508, 129721,\n",
       "        130225, 133411, 135207, 137798, 138246, 141517, 143754, 129356, 131988,\n",
       "        133442, 135049, 137706, 139295, 141301, 143426, 128096, 131763, 133583,\n",
       "        135882, 138042, 139177, 140579, 143748, 128777, 132076, 133283, 135913,\n",
       "        136326, 139039, 140993, 143849, 128505, 131396, 133917, 135805, 136482,\n",
       "        139801, 140598, 143465, 128526, 131624, 134083, 135933, 137119, 138807,\n",
       "        142219, 143542, 129146, 131526, 133208, 134889, 136540, 138609, 140881,\n",
       "        143960, 129235, 130842, 134120, 135056, 136239, 139371, 142065, 143465,\n",
       "        128981, 131914, 132919, 136140, 137531, 139670, 140380, 143349, 129708,\n",
       "        130302, 132534, 136098, 137274, 138754, 142050, 143112, 129473, 130871,\n",
       "        133060, 136084, 137224, 139800, 141985, 142724, 129358, 130417, 132419,\n",
       "        134201, 137032, 139209, 142248, 144188, 128502, 131104, 133722, 134308,\n",
       "        136773, 139270, 141800, 144344, 129268, 131552, 132208, 134758, 137754,\n",
       "        138645, 140963, 143949, 128885, 130731, 133042, 134676, 138067, 138689,\n",
       "        141153, 142518, 129558, 130562, 132762, 135147, 136479, 138505, 141154,\n",
       "        142954, 129252, 130302, 133623, 135723, 136773, 138820, 141661, 142916,\n",
       "        129260, 132080, 132886, 135635, 137689, 140076, 141626, 143059, 128304,\n",
       "        132035, 132754, 135963, 137388, 139734, 140330, 143591, 129631, 130914,\n",
       "        133663, 134275, 137724, 139944, 141107, 142904, 129565, 130486, 132722,\n",
       "        136154, 136729, 138547, 141040, 142487, 144644], dtype=torch.int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "CACHE_DIR = '/home/subhash/.cache/indri'\n",
    "def load_tokens(dataset_dir):\n",
    "    metadata_path = f\"{CACHE_DIR}/{dataset_dir}/annotation/metadata.jsonl\"\n",
    "    tokens_dir = os.path.join(CACHE_DIR, dataset_dir, 'tokens', 'mimi')\n",
    "    with open(metadata_path, 'r', encoding='utf-8') as file:\n",
    "        for line_number, line in enumerate(file, 1):\n",
    "            data = json.loads(line.strip())            \n",
    "            file_path = os.path.join(tokens_dir, data['id'] + '.npy')\n",
    "            \n",
    "            audio_tokens = np.load(file_path)\n",
    "            weave_audio = weave_tokens(audio_tokens.tolist())\n",
    "            yield data['raw_text'], weave_audio, data['speaker_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([128000,  39628,     11,  ..., 140557, 142493, 144644],\n",
      "       dtype=torch.int32)\n",
      "<|begin_of_text|>Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition[convert][spkr_unk][mimi][aco_1442][aco_3215][aco_4335][aco_6316][aco_8867][aco_11749][aco_14000][aco_15936][aco_1463][aco_3102][aco_4741][aco_6853][aco_9438][aco_11023][aco_12637][aco_15755][aco_12][aco_3327][aco_5098][aco_7331][aco_8508][aco_11952][aco_12220][aco_14360][aco_1133][aco_3558][aco_5358][aco_6739][aco_7940][aco_11184][aco_12999][aco_15792][aco_595][aco_2796][aco_5106][aco_7093][aco_8529][aco_11455][aco_13701][aco_15488][aco_1516][aco_2061][aco_5212][aco_6954][aco_8614][aco_10897][aco_13674][aco_14271]<|reserved_special_token_178|>[aco_2240][aco_4828][aco_6706][aco_8849][aco_10478][aco_12378][aco_15799][aco_1051][aco_3045][aco_4302][aco_7390][aco_8951][aco_11910][aco_12257][aco_15170][aco_1639][aco_2699][aco_4302][aco_6019][aco_8955][aco_11823][aco_12501][aco_15948][aco_1699][aco_2848][aco_5018][aco_7236][aco_8276][aco_11556][aco_12851][aco_14252][aco_1377][aco_2945][aco_4099][aco_7857][aco_8632][aco_10208][aco_12454][aco_15997][aco_515][aco_2083][aco_5832][aco_6442][aco_9266][aco_11856][aco_14011][aco_15956][aco_670][aco_2425][aco_5215][aco_6916][aco_9787][aco_10366][aco_12853][aco_14841][aco_1274][aco_1994][aco_4686][aco_6711][aco_9467][aco_10276][aco_12540][aco_15436][aco_1733][aco_2759][aco_5271][aco_6301][aco_9913][aco_11306][aco_13253][aco_14183][aco_1318][aco_3416][aco_4927][aco_6776][aco_9026][aco_10721][aco_12717][aco_16054][aco_1318][aco_3631][aco_5565][aco_7158][aco_9050][aco_10001][aco_12212][aco_15585][aco_466][aco_1914][aco_5193][aco_6968][aco_9149][aco_10985][aco_13654][aco_14271]<|reserved_special_token_136|>[aco_3601][aco_4382][aco_6809][aco_8835][aco_10527][aco_14059][aco_14945][aco_1689][aco_2947][aco_4347][aco_6079][aco_8821][aco_9990][aco_13675][aco_15317][aco_22][aco_2318][aco_4112][aco_7713][aco_9016][aco_10310][aco_13480][aco_15476][aco_853][aco_1951][aco_5839][aco_7063][aco_8790][aco_10921][aco_13748][aco_14291]<|reserved_special_token_21|>[aco_3315][aco_5851][aco_7695][aco_8724][aco_10321][aco_12909][aco_15491][aco_355][aco_3766][aco_4770][aco_7898][aco_9385][aco_9990][aco_12572][aco_15424]<|reserved_special_token_38|>[aco_2721][aco_5163][aco_6945][aco_9225][aco_11005][aco_12572][aco_14779][aco_366][aco_2205][aco_4426][aco_7872][aco_9048][aco_10332][aco_12926][aco_16039][aco_372][aco_3026][aco_5034][aco_6531][aco_9476][aco_10537][aco_12738][aco_15306][aco_1484][aco_3750][aco_4225][aco_6715][aco_9374][aco_12002][aco_12653][aco_16074][aco_316][aco_3460][aco_5374][aco_7713][aco_8527][aco_10657][aco_12682][aco_14713][aco_316][aco_2713][aco_5745][aco_7839][aco_9385][aco_10235][aco_13583][aco_15341][aco_89][aco_2343][aco_5832][aco_6930][aco_8026][aco_11198][aco_12297][aco_15611][aco_1733][aco_2949][aco_3909][aco_6296][aco_9795][aco_10079][aco_13518][aco_14740][aco_673][aco_3253][aco_5577][aco_6211][aco_8627][aco_10317][aco_12263][aco_14160][aco_673][aco_3839][aco_4299][aco_5905][aco_9520][aco_10122][aco_12466][aco_14094][aco_1520][aco_2202][aco_3975][aco_7713][aco_9526][aco_10908][aco_12320][aco_16062][aco_493][aco_2046][aco_5580][aco_6498][aco_9247][aco_11551][aco_13743][aco_15837][aco_57][aco_2575][aco_4032][aco_6356][aco_9488][aco_11312][aco_12116][aco_15229][aco_1741][aco_3202][aco_5839][aco_6479][aco_9071][aco_11243][aco_12259][aco_14104][aco_1315][aco_2995][aco_4558][aco_5990][aco_8020][aco_11781][aco_13532][aco_15187][aco_563][aco_2506][aco_5064][aco_6369][aco_8600][aco_11703][aco_12843][aco_14429][aco_982][aco_2713][aco_4812][aco_7805][aco_8340][aco_10641][aco_13755][aco_14945][aco_798][aco_2213][aco_5170][aco_7162][aco_8310][aco_11876][aco_12466][aco_15670][aco_798][aco_3315][aco_4561][aco_5917][aco_8654][aco_11544][aco_14024][aco_14600][aco_879][aco_3278][aco_5884][aco_6223][aco_8821][aco_11017][aco_13391][aco_15195][aco_856][aco_3496][aco_4589][aco_7332][aco_9430][aco_11433][aco_13435][aco_14945][aco_1137][aco_3639][aco_4952][aco_5986][aco_8351][aco_11134][aco_12470][aco_14306][aco_360][aco_3824][aco_4944][aco_6163][aco_8353][aco_10189][aco_13886][aco_15128][aco_1446][aco_2803][aco_5139][aco_7218][aco_8147][aco_11126][aco_13845][aco_16078][aco_737][aco_3315][aco_4509][aco_7241][aco_8145][aco_11458][aco_13811][aco_15837][aco_1291][aco_3338][aco_4066][aco_7852][aco_7978][aco_10785][aco_13823][aco_15837][aco_230][aco_3003][aco_4989][aco_7236][aco_8759][aco_10284][aco_13286][aco_15519][aco_230][aco_2848][aco_5018][aco_6632][aco_8851][aco_10038][aco_13008][aco_15260][aco_1783][aco_2035][aco_5018][aco_7236][aco_8517][aco_10539][aco_12358][aco_15353][aco_1712][aco_2035][aco_4989][aco_7236][aco_8417][aco_11014][aco_14010][aco_15824][aco_401][aco_2035][aco_5399][aco_6019][aco_9672][aco_11014][aco_13008][aco_15948][aco_408][aco_3303][aco_4731][aco_7471][aco_8483][aco_10884][aco_12711][aco_15587][aco_83][aco_2042][aco_5250][aco_6465][aco_8619][aco_10924][aco_13004][aco_14842][aco_83][aco_2682][aco_5742][aco_7104][aco_9771][aco_12025][aco_13535][aco_15810][aco_332][aco_3534][aco_4992][aco_7230][aco_8904][aco_10578][aco_12288][aco_15123][aco_1753][aco_1849][aco_5194][aco_6111][aco_8918][aco_10102][aco_13719][aco_14342][aco_654][aco_3383][aco_4437][aco_7024][aco_8576][aco_11997][aco_13035][aco_14945]<|reserved_special_token_24|>[aco_2549][aco_4878][aco_6734][aco_8239][aco_11896][aco_12301][aco_14945][aco_1064][aco_2842][aco_5155][aco_6436][aco_8417][aco_11918][aco_12192][aco_14093]<|reserved_special_token_139|>[aco_3430][aco_4987][aco_6241][aco_9864][aco_11024][aco_13968][aco_15143][aco_184][aco_2811][aco_5698][aco_6688][aco_9684][aco_11843][aco_12854][aco_14271]<|eom_id|>[aco_3631][aco_4107][aco_6122][aco_8653][aco_11566][aco_12398][aco_15967][aco_1466][aco_3473][aco_4592][aco_7668][aco_9477][aco_11859][aco_13053][aco_14108][aco_1526][aco_2930][aco_4091][aco_6586][aco_8181][aco_11660][aco_13068][aco_14279][aco_1394][aco_2353][aco_4358][aco_6399][aco_9061][aco_10868][aco_13007][aco_15058][aco_1359][aco_3278][aco_5583][aco_6042][aco_9858][aco_11746][aco_12110][aco_14945][aco_542][aco_2094][aco_5189][aco_7084][aco_7992][aco_10042][aco_12948][aco_15348][aco_661][aco_3023][aco_5374][aco_6773][aco_9082][aco_11543][aco_12886][aco_14191][aco_47][aco_3568][aco_5430][aco_7149][aco_9051][aco_11552][aco_13648][aco_15017][aco_441][aco_3331][aco_4230][aco_6335][aco_9202][aco_11501][aco_12729][aco_14487][aco_581][aco_2930][aco_4823][aco_7690][aco_8401][aco_10378][aco_13588][aco_14601][aco_102][aco_2341][aco_5815][aco_7649][aco_9050][aco_11744][aco_12869][aco_15667][aco_1626][aco_2682][aco_4001][aco_7852][aco_8894][aco_11319][aco_12925][aco_14648][aco_184][aco_2811][aco_4000][aco_6885][aco_8741][aco_11170][aco_12072][aco_15423][aco_1736][aco_3140][aco_5371][aco_6213][aco_8221][aco_11589][aco_12942][aco_14933][aco_1736][aco_2046][aco_4163][aco_6784][aco_9253][aco_11319][aco_13099][aco_15945][aco_331][aco_2723][aco_5691][aco_7770][aco_9763][aco_10858][aco_13589][aco_15467]<|reserved_special_token_170|>[aco_2684][aco_4050][aco_6679][aco_9574][aco_10562][aco_13435][aco_15338]<|reserved_special_token_170|>[aco_2811][aco_4541][aco_6549][aco_9616][aco_11505][aco_12654][aco_15285][aco_1371][aco_1848][aco_5055][aco_7711][aco_8445][aco_11779][aco_13759][aco_15850][aco_1274][aco_2796][aco_3966][aco_7200][aco_9543][aco_11809][aco_13984][aco_15732][aco_673][aco_2143][aco_4200][aco_6679][aco_9674][aco_11809][aco_12103][aco_15621][aco_1354][aco_3038][aco_4952][aco_6637][aco_8336][aco_11970][aco_12861][aco_15438][aco_1660][aco_2739][aco_5535][aco_6188][aco_9734][aco_10465][aco_12641][aco_15890][aco_267][aco_3592][aco_4523][aco_6579][aco_9575][aco_9990][aco_13343][aco_15409]<|reserved_special_token_205|>[aco_2676][aco_4192][aco_6579][aco_8775][aco_11656][aco_13563][aco_14103][aco_996][aco_3141][aco_5770][aco_7087][aco_8846][aco_10941][aco_12769][aco_14988][aco_1224][aco_3297][aco_5590][aco_7567][aco_9743][aco_10427][aco_12969][aco_15509][aco_1212][aco_2774][aco_4287][aco_7093][aco_9384][aco_10455][aco_13245][aco_14602][aco_1643][aco_2774][aco_5279][aco_7028][aco_8437][aco_11420][aco_12994][aco_15833][aco_295][aco_3430][aco_4001][aco_7806][aco_9802][aco_11017][aco_12703][aco_14231][aco_1777][aco_3230][aco_5764][aco_7824][aco_9001][aco_11828][aco_13796][aco_15143][aco_1777][aco_3322][aco_4789][aco_7771][aco_8642][aco_9990][aco_13187][aco_15502]<|reserved_special_token_75|>[aco_2516][aco_4738][aco_6234][aco_7976][aco_10498][aco_13933][aco_15037][aco_890][aco_3473][aco_3874][aco_7386][aco_9054][aco_10304][aco_12378][aco_15730][aco_528][aco_2607][aco_5785][aco_7534][aco_9160][aco_11028][aco_12339][aco_15049][aco_1039][aco_3020][aco_3887][aco_6526][aco_8392][aco_11200][aco_13442][aco_15635]<|reserved_special_token_191|>[aco_3216][aco_5020][aco_6450][aco_9101][aco_11464][aco_12041][aco_14653][aco_853][aco_3605][aco_5839][aco_5891][aco_9959][aco_10915][aco_12189][aco_15975][aco_12][aco_3460][aco_4070][aco_6951][aco_8837][aco_10284][aco_13642][aco_15253]<|start_header_id|>[aco_3407][aco_4602][aco_6182][aco_8805][aco_11153][aco_12059][aco_15019]<|start_header_id|>[aco_2578][aco_5585][aco_7152][aco_8514][aco_10207][aco_13958][aco_14861][aco_1525][aco_3311][aco_3996][aco_7149][aco_9699][aco_12030][aco_12869][aco_15253][aco_1223][aco_2482][aco_4212][aco_7425][aco_8896][aco_10375][aco_13379][aco_15492][aco_1274][aco_3507][aco_4609][aco_7433][aco_8224][aco_11222][aco_12911][aco_15002][aco_1274][aco_3407][aco_4522][aco_6351][aco_9155][aco_11254][aco_13487][aco_14857]<|reserved_special_token_138|>[aco_3062][aco_5484][aco_7402][aco_8907][aco_10591][aco_12755][aco_15804][aco_1782][aco_2656][aco_4558][aco_6758][aco_8215][aco_10614][aco_13583][aco_15284][aco_728][aco_2947][aco_4662][aco_6067][aco_9064][aco_10410][aco_13201][aco_15195][aco_1147][aco_3068][aco_4676][aco_6436][aco_9078][aco_10112][aco_12566][aco_14750][aco_350][aco_3827][aco_5203][aco_6085][aco_8682][aco_11439][aco_12160][aco_15186][aco_1123][aco_3023][aco_5864][aco_7428][aco_7999][aco_11548][aco_12510][aco_14750][aco_1584][aco_3322][aco_5413][aco_6061][aco_8929][aco_10591][aco_12442][aco_14945][aco_916][aco_2631][aco_4435][aco_7051][aco_8050][aco_10663][aco_12420][aco_15750][aco_1424][aco_2240][aco_5568][aco_7742][aco_9826][aco_11781][aco_12594][aco_15347][aco_906][aco_2699][aco_5466][aco_6609][aco_7980][aco_11851][aco_13366][aco_14382][aco_980][aco_2230][aco_4466][aco_7052][aco_9552][aco_11182][aco_12301][aco_14237][stop]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115641/1348805275.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  text_tokens = torch.tensor(tokenizer.encode(text), dtype=torch.int32).view(-1).clone().detach()\n",
      "/tmp/ipykernel_115641/1348805275.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  convert_tokens = torch.tensor(tokenizer.encode(CONVERT, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
      "/tmp/ipykernel_115641/1348805275.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  continue_tokens = torch.tensor(tokenizer.encode(CONTINUE, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
      "/tmp/ipykernel_115641/1348805275.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  speaker_tokens = torch.tensor(tokenizer.encode(speaker, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
      "/tmp/ipykernel_115641/1348805275.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mimi_tokens = torch.tensor(tokenizer.encode(MIMI, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n",
      "/tmp/ipykernel_115641/1348805275.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  stop_tokens = torch.tensor(tokenizer.encode(COMMON_STOP, add_special_tokens=False), dtype=torch.int32).view(-1).clone().detach()\n"
     ]
    }
   ],
   "source": [
    "dataset = 'lj_speech'\n",
    "for raw_text, audio_tokens, speaker in load_tokens(dataset_dir=dataset):\n",
    "    with open('allowed_speakers.jsonl', 'r', encoding='utf-8') as file:\n",
    "        allowed_speakers = [json.loads(line.strip()) for line in file]\n",
    "    entry = next((item for item in allowed_speakers if item['dataset'] == dataset and item['speaker'] == speaker), None)\n",
    "    if entry:\n",
    "        combined = entry['combined']\n",
    "    else:\n",
    "        combined = DEFAULT_SPEAKER\n",
    "    result = append_tokens(raw_text, audio_tokens, speaker=combined)\n",
    "    print(result)\n",
    "    print(tokenizer.decode(result))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (replace this with your actual data)\n",
    "\n",
    "\n",
    "# Function to get combined ID based on dataset and speaker\n",
    "def get_combined_id(data, dataset, speaker):\n",
    "    entry = next((item for item in data if item['dataset'] == dataset and item['speaker'] == speaker), None)\n",
    "    return entry['combined'] if entry else None\n",
    "\n",
    "# Example usage\n",
    "dataset_input = \"mls_eng_10k\"\n",
    "speaker_input = \"2156\"\n",
    "combined_id = get_combined_id(data, dataset_input, speaker_input)\n",
    "\n",
    "print(combined_id)  # Output: [spkr_mls_eng_10k_2156]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_pickle_file(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "pickle_file_path = 'tokens/lj_speech_tokens.pkl'  \n",
    "data = load_pickle_file(pickle_file_path)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_model import Llama, LlamaConfig\n",
    "def calculate_parameters(config: LlamaConfig) -> int:\n",
    "    total_params = 0\n",
    "\n",
    "    total_params += config.vocab_size * config.dim \n",
    "\n",
    "    for _ in range(config.n_layers):\n",
    "        total_params += (config.dim * config.n_heads * (config.dim // config.n_heads)) * 3  # wq, wk, wv\n",
    "        total_params += (config.n_heads * (config.dim // config.n_heads) * config.dim)  # wo\n",
    "\n",
    "        hidden_dim = int(4 * config.dim)  # Assuming hidden_dim is 4 * dim\n",
    "        total_params += (config.dim * hidden_dim) + (hidden_dim * config.dim)  # w1 and w2\n",
    "        total_params += (config.dim * hidden_dim)  # w3\n",
    "\n",
    "    total_params += config.dim * 2 * config.n_layers\n",
    "\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1101588480\n"
     ]
    }
   ],
   "source": [
    "config = LlamaConfig(\n",
    "    dim=2048,\n",
    "    n_layers=12,\n",
    "    n_heads=16,\n",
    "    vocab_size=144645,\n",
    "    max_seq_len=2048\n",
    ")\n",
    "total_params = calculate_parameters(config)\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subhash/miniconda3/lib/python3.12/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor with the maximum length: tensor([128000,     33,   4361,  ..., 141628, 143887, 144644],\n",
      "       dtype=torch.int32)\n",
      "Maximum length: 1063\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the pickle file\n",
    "with open('tokens/lj_speech_tokens.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Initialize variables to keep track of the maximum length tensor\n",
    "max_length = 0\n",
    "max_tensor = None\n",
    "\n",
    "# Iterate through the tensors in the data\n",
    "for tensor in data:\n",
    "    length = tensor.shape[0]  # Assuming the length is the first dimension\n",
    "    if length > max_length:\n",
    "        max_length = length\n",
    "        max_tensor = tensor\n",
    "\n",
    "# Output the tensor with the maximum length\n",
    "print(\"Tensor with the maximum length:\", max_tensor)\n",
    "print(\"Maximum length:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "from llama_model import Llama, LlamaConfig\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, tokens_file, max_token=144641):\n",
    "        with open(tokens_file, 'rb') as f:\n",
    "            all_tokens = pickle.load(f)\n",
    "        \n",
    "        self.tokens = [\n",
    "            seq.tolist() for seq in all_tokens\n",
    "        ]\n",
    "        \n",
    "        print(f\"Total sequences: {len(self.tokens)}\")\n",
    "        print(f\"Max sequence length after filtering: {max(len(seq) for seq in self.tokens)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.tokens[idx]\n",
    "        \n",
    "        # Find the index of the max_token\n",
    "        if 144641 in sequence:\n",
    "            split_idx = sequence.index(144641) + 1\n",
    "        else:\n",
    "            split_idx = len(sequence)\n",
    "        \n",
    "        # Split the sequence into input and output\n",
    "        input_seq = sequence[:split_idx]\n",
    "        output_seq = sequence[split_idx:]\n",
    "        \n",
    "        # Convert to tensor directly \n",
    "        input_seq = torch.tensor(input_seq, dtype=torch.long)\n",
    "        output_seq = torch.tensor(output_seq, dtype=torch.long)\n",
    "        \n",
    "        return input_seq, output_seq\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Unpack the batch\n",
    "    inputs, targets = zip(*batch)\n",
    "    \n",
    "    # Ensure inputs and targets have matching lengths\n",
    "    max_input_len = max(len(inp) for inp in inputs)\n",
    "    max_target_len = max(len(tgt) for tgt in targets)\n",
    "    max_len = min(max_input_len, max_target_len)  # Limit to shorter sequence\n",
    "    \n",
    "    # Pad and truncate sequences\n",
    "    inputs_padded = torch.stack([\n",
    "        F.pad(inp[:max_len], (0, max_len - len(inp[:max_len])), value=0) \n",
    "        for inp in inputs\n",
    "    ])\n",
    "    \n",
    "    targets_padded = torch.stack([\n",
    "        F.pad(tgt[:max_len], (0, max_len - len(tgt[:max_len])), value=-100)  # -100 for ignore_index in loss\n",
    "        for tgt in targets\n",
    "    ])\n",
    "\n",
    "    # Create attention mask\n",
    "    attention_mask = (inputs_padded != 0).float()\n",
    "    \n",
    "    return inputs_padded, targets_padded, attention_mask\n",
    "\n",
    "def get_vocab_size(tokens_file):\n",
    "    with open(tokens_file, 'rb') as f:\n",
    "        tokens = pickle.load(f)\n",
    "    \n",
    "    # Find the maximum token, ensuring it doesn't exceed max_token\n",
    "    max_token_in_data = max(max(seq) for seq in tokens)\n",
    "    return max_token_in_data + 1\n",
    "\n",
    "def train_model(model, dataloader, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    device = next(model.parameters()).device\n",
    "  \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0  # Initialize total_loss for each epoch\n",
    "        for batch in dataloader:\n",
    "            inputs, targets, attention_mask = batch\n",
    "            inputs, targets, attention_mask = inputs.to(device), targets.to(device), attention_mask.to(device)\n",
    "            \n",
    "            print(f\"Inputs shape: {inputs.shape}\")\n",
    "            print(f\"Targets shape: {targets.shape}\")\n",
    "            if attention_mask is not None:\n",
    "                print(f\"Attention mask shape: {attention_mask.shape}\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = model.forward_loss(inputs, targets, attention_mask=attention_mask)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    tokens_file = 'tokens/lj_speech_tokens.pkl'\n",
    "    max_token = 144641  # Specific max token you mentioned\n",
    "    \n",
    "    # Get vocabulary size based on the max token\n",
    "    vocab_size = get_vocab_size(tokens_file)\n",
    "    print(f\"Detected Vocabulary Size: {vocab_size}\")\n",
    "    \n",
    "    config = LlamaConfig(\n",
    "        dim=512,  # Reduced model size for faster training\n",
    "        n_layers=4,\n",
    "        n_heads=8,\n",
    "        vocab_size=vocab_size,  # Important: use exact vocab size\n",
    "        max_seq_len=256,  # Increased to accommodate longer sequences\n",
    "        multiple_of=256,\n",
    "        use_scaled_rope=True\n",
    "    )\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = TokenDataset(tokens_file, max_token)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=2,\n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    torch.manual_seed(42)  # For reproducibility\n",
    "    model = Llama(config).cuda()\n",
    "    \n",
    "    # Configure optimizer\n",
    "    optimizer = model.configure_optimizers(\n",
    "        weight_decay=0.01, \n",
    "        learning_rate=1e-4, \n",
    "        betas=(0.9, 0.95), \n",
    "        device_type='cuda'\n",
    "    )\n",
    "    \n",
    "    # Enable anomaly detection for debugging\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(model, dataloader, optimizer, num_epochs=10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inferencing the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2237071/1634869734.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('models/llama_model_epoch_100.pth')\n",
      "/home/meraki/miniconda3/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/tmp/ipykernel_2237071/1634869734.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_seq = torch.tensor(input_seq, dtype=torch.long)\n",
      "/tmp/ipykernel_2237071/1634869734.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output_seq = torch.tensor(output_seq, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2663, Accuracy: 0.9007\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from llama_model import Llama, LlamaConfig\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, tokens_file):\n",
    "        with open(tokens_file, 'rb') as f:\n",
    "            self.tokens = pickle.load(f)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.tokens[idx]\n",
    "        special_token = 144641\n",
    "        if special_token in sequence:\n",
    "            split_idx = (sequence == special_token).nonzero(as_tuple=True)[0].item() + 1\n",
    "        else:\n",
    "            split_idx = len(sequence)\n",
    "        input_seq = sequence[:split_idx]\n",
    "        output_seq = sequence[split_idx:]\n",
    "        input_seq = torch.tensor(input_seq, dtype=torch.long)\n",
    "        output_seq = torch.tensor(output_seq, dtype=torch.long)\n",
    "        return input_seq, output_seq\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    max_input_len = max(len(inp) for inp in inputs)\n",
    "    max_target_len = max(len(tgt) for tgt in targets)\n",
    "    max_len = min(max_input_len, max_target_len)\n",
    "    inputs_padded = torch.stack([F.pad(inp[:max_len], (0, max_len - len(inp[:max_len])), value=0) for inp in inputs])\n",
    "    targets_padded = torch.stack([F.pad(tgt[:max_len], (0, max_len - len(tgt[:max_len])), value=-100) for tgt in targets])\n",
    "    attention_mask = (inputs_padded != 0).float()\n",
    "    return inputs_padded, targets_padded, attention_mask\n",
    "\n",
    "config = LlamaConfig(\n",
    "    dim=1024,  \n",
    "    n_layers=12,\n",
    "    n_heads=16,\n",
    "    vocab_size=144646,  \n",
    "    max_seq_len=2048,  \n",
    "    multiple_of=2048,\n",
    "    use_scaled_rope=True\n",
    ")\n",
    "model = Llama(config).cuda()\n",
    "state_dict = torch.load('models/llama_model_epoch_100.pth')\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "tokens_file = 'tokens/lj_speech_tokens.pkl'\n",
    "dataset = TokenDataset(tokens_file)\n",
    "dataloader = DataLoader(dataset, batch_size=24, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "total_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        inputs, targets, attention_mask = batch\n",
    "        inputs, targets, attention_mask = inputs.cuda(), targets.cuda(), attention_mask.cuda()\n",
    "        \n",
    "        start_pos = 0\n",
    "        seqlen = inputs.size(1)\n",
    "        freqs_cis = model._prepare_rotary_embeddings(inputs, seqlen)\n",
    "        mask = torch.triu(torch.full((seqlen, seqlen), float('-inf'), device=inputs.device), diagonal=1)\n",
    "        \n",
    "        h = model.tok_embeddings(inputs)\n",
    "        for layer in model.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        \n",
    "        h = model.norm(h)\n",
    "        outputs = model.output(h)\n",
    "        \n",
    "        loss = model.forward_loss(inputs, targets, attention_mask=attention_mask)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, dim=-1)\n",
    "        correct = (predicted == targets).float() * attention_mask\n",
    "        total_correct += correct.sum().item()\n",
    "        total_tokens += attention_mask.sum().item()\n",
    "\n",
    "average_loss = total_loss / len(dataloader)\n",
    "accuracy = total_correct / total_tokens\n",
    "\n",
    "print(f\"Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2211974/1927633890.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('models/llama_model_epoch_100.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Llama(\n",
       "  (tok_embeddings): Embedding(144646, 1024)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (wk): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (wv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "        (w2): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (w3): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=1024, out_features=144646, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from llama_model import Llama, LlamaConfig\n",
    "\n",
    "config = LlamaConfig(\n",
    "    dim=1024,  \n",
    "    n_layers=12,\n",
    "    n_heads=16,\n",
    "    vocab_size=144646,  \n",
    "    max_seq_len=2048,  \n",
    "    multiple_of=2048,\n",
    "    use_scaled_rope=True\n",
    ")\n",
    "model = Llama(config).cuda()\n",
    "state_dict = torch.load('models/llama_model_epoch_100.pth')\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_output_tokens(model, input_tokens, max_new_tokens, temperature=0.8, top_k=50, stop_token=144644):\n",
    "    idx = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0).cuda()\n",
    "    start_pos = 0\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer.attention.cache_k = torch.zeros_like(layer.attention.cache_k)\n",
    "        layer.attention.cache_v = torch.zeros_like(layer.attention.cache_v)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx if idx.size(1) <= model.config.max_seq_len else idx[:, -model.config.max_seq_len:]        \n",
    "        h = model.tok_embeddings(idx_cond)        \n",
    "        freqs_cis = model._prepare_rotary_embeddings(h, idx_cond.size(1))        \n",
    "        seqlen = idx_cond.size(1)\n",
    "        mask = torch.triu(torch.full((seqlen, seqlen), float('-inf'), device=h.device), diagonal=1)\n",
    "        \n",
    "        for layer in model.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        \n",
    "        logits = model.norm(h)\n",
    "        logits = model.output(logits)[:, -1, :] / temperature\n",
    "        \n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = float('-inf')\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        if stop_token is not None and idx_next.item() == stop_token:\n",
    "            break\n",
    "        \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        start_pos += 1\n",
    "\n",
    "    generated_tokens = idx[:, len(input_tokens):]\n",
    "    return generated_tokens.squeeze().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 tensor([140589, 134447, 131458,  ..., 129250, 129304, 129670])\n"
     ]
    }
   ],
   "source": [
    "input_token = [128000, 135, 286, 790, 144642, 144645, 144641]\n",
    "output = generate_output_tokens(model, input_token, max_new_tokens=1024)\n",
    "print(len(output), output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deflatten_tokens(tokens, n_codebooks, per_codebook_size):\n",
    "    arr = []\n",
    "    for i in range(n_codebooks):\n",
    "        arr.append(tokens[i::n_codebooks])\n",
    "    acoustic_tokens = np.stack(arr)\n",
    "    return acoustic_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([140589, 134447, 131458,  ..., 129250, 129304, 129670])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ac = deflatten_tokens(tokens=output,n_codebooks=8,per_codebook_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[140589, 128933, 129304, ..., 129250, 128933, 128363],\n",
       "       [134447, 130207, 129953, ..., 128727, 129304, 129953],\n",
       "       [131458, 128727, 129947, ..., 129304, 129250, 129670],\n",
       "       ...,\n",
       "       [129670, 129250, 128363, ..., 128727, 129437, 129250],\n",
       "       [129437, 129901, 128363, ..., 129282, 129250, 129304],\n",
       "       [130207, 129947, 129250, ..., 129437, 129304, 129670]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = ac-128000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ac type: <class 'numpy.ndarray'>\n",
      "ac shape: (8, 128)\n",
      "ac dtype: int64\n",
      "ac sample values: [[12589   933  1304  1953  1054  1437   363  1947   363  2207   727  1250\n",
      "   1953  1947  1054   363  1371  1947   363  1947  2207  1304  1670  1947\n",
      "   1670  1947   363  1670   933  2273   868  2273  1670  1054  1282  1670\n",
      "   1437  1953   363  1371  1054  1393  1250  1304  1670  1304  1670  1304\n",
      "    933  1953  1947  1670  1250  1282  1250  2273  1670  1953  1670  2207\n",
      "   1250   363  1304  1250  1953   363  1953  1250  1953  2207  1250   363\n",
      "   1670  1670   363  1947  1670  1054  1304  2207   933   933  1304  1304\n",
      "   1250   727  1437  1947  1250  1437  1437   363  1437  1947  1670  1437\n",
      "   2273  2273  1437   933  1953  1953  1054  1437  1304   363  1953   363\n",
      "   2273  1437  1282  1250  1230   363  1670  1304  1250  2207  1437  1953\n",
      "   1947  1250   933  1282  1670  1250   933   363]\n",
      " [ 6447  2207  1953  2273  1054   727  1437  1947  1947   398  1054  1054\n",
      "   1670  1953  1947  1250  1670  2207  1250  1953  1393  1304  1953   363\n",
      "   2273  1304   933  1304  1250  1670  2207   727  1670  1953   933  1250\n",
      "   2207  1250  1953  1250  1437   363   363  1054  1250  1947  1947   363\n",
      "   1953  1953  1437  1437  1947   933  1250  1250  1250  1304  1947  1054\n",
      "   1230  1054  1250  1304  1953  1947   363  1250  1953   933  1054  2207\n",
      "   1304  1437  1947  1947  1304   363  1670  1947  1670  1947  1953  1054\n",
      "   2273  1953  1953  1054   363   933  2273  1947  1670  1437  1953  1250\n",
      "   1304  2273  1437  1250  1282  1953  1304   727   363  1947  1953  2273\n",
      "   1304  1250  1230  1282  1304  2207  1250  1054  1670  1282   933  1670\n",
      "   1953  1250  1947   363  2207   727  1304  1953]\n",
      " [ 3458   727  1947  1947  1250  1947   571  1054  1947  1304  1250  1670\n",
      "   1250  1250  2273  2114  1670   363  1670  1670  1437  1304  2273  1054\n",
      "   1250  1670   398  1304   933   868  1953   571  1054  1304  1250  1953\n",
      "   2207  1304  1304   363   363  1947  1947  1250  1304  1953  1947  1953\n",
      "   1947   933  2207  1670  1250  1250   868  1947   933  1437   727  2273\n",
      "   1670  2207  1393  1670   933  1371  1437   933   363  1953  1437  1304\n",
      "   1953  1250  1304  2273  1250  1670   316  1947  1953  1250  1371  1304\n",
      "   1947   363  1304  1437  1953  1953  1953  1054  1393  1437  1947  1670\n",
      "   1670   727   363  1953  1437  1953  1230  1670  2273   868  1230  1670\n",
      "   1054  1250  1670  1054  1670  1670  1947  1250   571   933   363   363\n",
      "   2273  1670  2273   933  1953  1304  1250  1670]\n",
      " [ 3762  1953  1393  1054  1953  1304  1250  1670  2273   363  1953  1054\n",
      "   2273  1947  1304  1371  2207  1304  1371  1250   933  1670  1304  1304\n",
      "   1670  1250   571  1304  1947  1250   868   933  1393  1437  2207   933\n",
      "   1947  1953  1953  1953  1953   933  2207  1437  1670  1304  1953   363\n",
      "    933  1947   363  1670  1670  1250  1947   363  1304   933  1304  1304\n",
      "   1054   933   363  1304  1250  1947  1947  2273  1304  1304  1670   868\n",
      "   1304  1250  1947   363  2207   933  1304  1670  1304  1947  1250  1947\n",
      "    363  1250  1304  1437  1230   571   933  1437   363  1947  1304  1670\n",
      "   1947  1953  1393  1437  1947  1670  1250   363   363  2273   363  1304\n",
      "   1947  1670  1670  1953  1393  2207  2207   727  1282  1947  1250  1670\n",
      "   1250  1437  1304  1054  1054  1250  1304  1054]\n",
      " [ 2273  1670  2273  1250  1393   316  1947  1250  1947  1250  1304  1250\n",
      "   1250   363  1947  1953  1282  1304  1670  1437   363   868  1953  1304\n",
      "   1670  1670  1250  1250  1953  2207  1953  1250  1304   727  1230  1953\n",
      "    933  1947   933  1947  2207  1054   933  2207  2273  1250   868  1304\n",
      "   1947   363  1054  1250  1437  1670   727  1250  1393  1947   363   363\n",
      "   2273  1304  1670   363  1304  1670  1670   363  1054  1304  1670   363\n",
      "   1304  1947  1304   363  1304  1282  1947  1250  1054  1250  2273   868\n",
      "    933  1371  1437   316  1947   933  1953  1437  1947  1953  1250  2273\n",
      "   1304  2207   363  1947  1953  1670  1250  1250  1054  1304  1670  1250\n",
      "   1947  1437  1670  1250  1953  1371  1304  2114  1371  1953  1250  1250\n",
      "   2207   363  1953  1250  1250  1393  1304  1304]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"ac type: {type(ac)}\")\n",
    "print(f\"ac shape: {ac.shape}\")\n",
    "print(f\"ac dtype: {ac.dtype}\")\n",
    "print(f\"ac sample values: {ac[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape before decode: torch.Size([1, 1024])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor shape before decode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mac_tensor\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Now try decoding\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m audio \u001b[38;5;241m=\u001b[39m mimi_tokenizer\u001b[38;5;241m.\u001b[39mdecode(ac_tensor)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Save the audio\u001b[39;00m\n\u001b[1;32m     15\u001b[0m torchaudio\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.wav\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     17\u001b[0m     audio, \n\u001b[1;32m     18\u001b[0m     sample_rate\u001b[38;5;241m=\u001b[39mmimi_tokenizer\u001b[38;5;241m.\u001b[39msampling_rate,\n\u001b[1;32m     19\u001b[0m     compression\u001b[38;5;241m=\u001b[39mtorchaudio\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mCodecConfig(bit_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128000\u001b[39m)\n\u001b[1;32m     20\u001b[0m )\n",
      "Cell \u001b[0;32mIn[18], line 41\u001b[0m, in \u001b[0;36mMimiTokenizer.decode\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokens\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m---> 41\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mexpand_dims(tokens, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     42\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecode(tokens)\n\u001b[1;32m     43\u001b[0m     waveform \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39maudio_values\u001b[38;5;241m.\u001b[39mcpu()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# If ac_tensor is 1D, add a dimension to make it 2D\n",
    "if ac_tensor.ndim == 1:\n",
    "    ac_tensor = ac_tensor.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "# Ensure it's a long tensor\n",
    "ac_tensor = ac_tensor.long()\n",
    "\n",
    "# Verify the shape before decoding\n",
    "print(f\"Tensor shape before decode: {ac_tensor.shape}\")\n",
    "\n",
    "# Now try decoding\n",
    "audio = mimi_tokenizer.decode(ac_tensor)\n",
    "\n",
    "# Save the audio\n",
    "torchaudio.save(\n",
    "    'test.wav', \n",
    "    audio, \n",
    "    sample_rate=mimi_tokenizer.sampling_rate,\n",
    "    compression=torchaudio.io.CodecConfig(bit_rate=128000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def flatten_tokens(arr: torch.tensor,\n",
    "                   per_codebook_size: int):\n",
    "    \n",
    "    c, n = arr.shape\n",
    "    i_values = np.arange(c) * per_codebook_size\n",
    "    arr += i_values.reshape(c, 1)\n",
    "    flat_arr = arr.reshape(c * n, order='F')\n",
    "    return flat_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deflatten_tokens(tokens, n_codebooks, per_codebook_size):\n",
    "    arr = []\n",
    "    for i in range(n_codebooks):\n",
    "        arr.append(tokens[i::n_codebooks] - per_codebook_size * i)\n",
    "    acoustic_tokens = np.stack(arr)\n",
    "    return acoustic_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1063"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def load_pickle_file(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "pickle_file_path = 'tokens/lj_speech_tokens.pkl'  \n",
    "data = load_pickle_file(pickle_file_path)\n",
    "maxlen = 0\n",
    "for item in data:\n",
    "    maxlen = max(maxlen,len(item))\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([129536, 135589, 132156,  ..., 129692, 129698, 128678])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor([128000,    258,   1694,  71561,   6617,     13, 144642, 144645, 144641,\n",
    "        128678, 130835, 132339, 134203, 137663, 139106, 142070, 142354, 129536,\n",
    "        130238, 132125, 134707, 136590, 138658, 141753, 142495, 128362, 131923,\n",
    "        133219, 135748, 137040, 138904, 140309, 142608, 129057, 131923, 132339,\n",
    "        134833, 136254, 138522, 141680, 143399, 128389, 131442, 133814, 135748,\n",
    "        136937, 139761, 142242, 143471, 129558, 130834, 133087, 135506, 137091,\n",
    "        139920, 142253, 143824, 129054, 130238, 133462, 135701, 136499, 138697,\n",
    "        141120, 144007, 129343, 130297, 132950, 135831, 137259, 139195, 141398,\n",
    "        143899, 128321, 131400, 133395, 134382, 136325, 139199, 140550, 143850,\n",
    "        129839, 131895, 132960, 136035, 137376, 139298, 142203, 144094, 128902,\n",
    "        130505, 132643, 135349, 136999, 139242, 140509, 142800, 129389, 131868,\n",
    "        134110, 134746, 137426, 138776, 141839, 143836, 129389, 131787, 132564,\n",
    "        136076, 137925, 138423, 141994, 142439, 128483, 131513, 133818, 135280,\n",
    "        136566, 139729, 141788, 143399, 129003, 130468, 132238, 134491, 137093,\n",
    "        140108, 142029, 144369, 128722, 131481, 132955, 135161, 136952, 139786,\n",
    "        142080, 142402, 128049, 130442, 133451, 136184, 137869, 139712, 140897,\n",
    "        144011, 129407, 132080, 132740, 134385, 136581, 139019, 140432, 142363,\n",
    "        128653, 131845, 134115, 135469, 137923, 138431, 141712, 143732, 128283,\n",
    "        130951, 133686, 135985, 136323, 139514, 140513, 143179, 129655, 130807,\n",
    "        133689, 135412, 136692, 139551, 141882, 142395, 129287, 130868, 133245,\n",
    "        135905, 136533, 139764, 140614, 142874, 129120, 130955, 133112, 134286,\n",
    "        137134, 138251, 141905, 142930, 130042, 130364, 132879, 135492, 136612,\n",
    "        140107, 141107, 144344, 144644], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text vocab size 128000\n",
      "[aco_1535][aco_9662][aco_4155][aco_2433][aco_3836][aco_471][aco_142][aco_1048][aco_1137][aco_994][aco_994][aco_798][aco_471][aco_1048][aco_1026][aco_1697][aco_1691][aco_1048][aco_1697][aco_1181][aco_1115][aco_677][aco_1137][aco_1137][aco_1181][aco_1048][aco_107][aco_1691][aco_1181][aco_798][aco_2017][aco_107][aco_1691][aco_107][aco_677][aco_107][aco_2017][aco_107][aco_994][aco_1048][aco_798][aco_2017][aco_1691][aco_612][aco_1691][aco_612][aco_2017][aco_798][aco_1691][aco_994][aco_1697][aco_107][aco_994][aco_994][aco_107][aco_107][aco_1026][aco_1181][aco_1181][aco_107][aco_1048][aco_994][aco_677][aco_1697][aco_1697][aco_1951][aco_2017][aco_1691][aco_677][aco_2017][aco_798][aco_1181][aco_994][aco_1697][aco_677][aco_1951][aco_471][aco_1137][aco_994][aco_1691][aco_1048][aco_1697][aco_1697][aco_107][aco_1048][aco_1115][aco_1697][aco_1026][aco_1691][aco_1181][aco_60][aco_1691][aco_994][aco_798][aco_1691][aco_1181][aco_1115][aco_1137][aco_1048][aco_994][aco_1414][aco_1951][aco_107][aco_1691][aco_1691][aco_677][aco_677][aco_1691][aco_107][aco_1048][aco_1414][aco_994][aco_798][aco_994][aco_107][aco_1414][aco_994][aco_2017][aco_1048][aco_1115][aco_142][aco_1048][aco_1951][aco_677][aco_1697][aco_1048][aco_1181][aco_107][aco_1691][aco_1181][aco_994][aco_1414][aco_798][aco_107][aco_1137][aco_1048][aco_1691][aco_612][aco_798][aco_1414][aco_677][aco_1697][aco_1951][aco_1414][aco_994][aco_107][aco_1414][aco_1414][aco_994][aco_798][aco_1048][aco_677][aco_1951][aco_60][aco_1691][aco_1048][aco_1414][aco_1137][aco_315][aco_994][aco_994][aco_994][aco_1691][aco_107][aco_471][aco_1181][aco_994][aco_798][aco_1414][aco_1414][aco_1414][aco_1181][aco_994][aco_1951][aco_1414][aco_994][aco_1414][aco_471][aco_1414][aco_677][aco_2017][aco_994][aco_1026][aco_612][aco_677][aco_1048][aco_612][aco_994][aco_994][aco_1697][aco_2017][aco_994][aco_1691][aco_1137][aco_1697][aco_1048][aco_994][aco_1181][aco_1414][aco_1181][aco_1048][aco_1414][aco_798][aco_1414][aco_1048][aco_1691][aco_1414][aco_798][aco_1181][aco_798][aco_1414][aco_471][aco_107][aco_1048][aco_1048][aco_1691][aco_1951][aco_1697][aco_677][aco_798][aco_1115][aco_1048][aco_2017][aco_994][aco_994][aco_1691][aco_1181][aco_1048][aco_677][aco_994][aco_1414][aco_1414][aco_677][aco_994][aco_1137][aco_107][aco_1026][aco_994][aco_1181][aco_1181][aco_1026][aco_974][aco_1951][aco_1691][aco_1181][aco_1691][aco_994][aco_2017][aco_1414][aco_994][aco_1414][aco_798][aco_471][aco_1951][aco_1181][aco_1048][aco_1691][aco_471][aco_1697][aco_1181][aco_1697][aco_1697][aco_677][aco_994][aco_1951][aco_1951][aco_1048][aco_107][aco_1691][aco_1048][aco_677][aco_1048][aco_1048][aco_1048][aco_1697][aco_1048][aco_677][aco_677][aco_994][aco_994][aco_1691][aco_315][aco_2017][aco_1697][aco_107][aco_994][aco_1048][aco_994][aco_1691][aco_1048][aco_677][aco_2017][aco_677][aco_1691][aco_798][aco_1691][aco_1115][aco_677][aco_1181][aco_1115][aco_1691][aco_1026][aco_677][aco_994][aco_1697][aco_994][aco_1697][aco_1691][aco_107][aco_2017][aco_994][aco_1691][aco_1414][aco_994][aco_994][aco_1115][aco_1414][aco_1691][aco_677][aco_1414][aco_1697][aco_677][aco_677][aco_1951][aco_1697][aco_1048][aco_1951][aco_107][aco_1048][aco_798][aco_1697][aco_1181][aco_994][aco_612][aco_471][aco_1691][aco_994][aco_1697][aco_1414][aco_677][aco_1048][aco_1697][aco_798][aco_994][aco_798][aco_994][aco_2017][aco_1181][aco_2017][aco_1048][aco_1048][aco_60][aco_677][aco_677][aco_1414][aco_1414][aco_994][aco_1697][aco_107][aco_994][aco_1181][aco_1951][aco_2017][aco_798][aco_612][aco_677][aco_994][aco_1181][aco_1048][aco_677][aco_1697][aco_1691][aco_1181][aco_1414][aco_677][aco_1048][aco_677][aco_315][aco_677][aco_677][aco_1414][aco_994][aco_1048][aco_1697][aco_994][aco_1691][aco_1048][aco_1181][aco_1697][aco_471][aco_1181][aco_107][aco_1181][aco_798][aco_677][aco_798][aco_1691][aco_1181][aco_994][aco_1951][aco_1697][aco_1181][aco_2017][aco_798][aco_1858][aco_994][aco_1951][aco_974][aco_1181][aco_1181][aco_1181][aco_1697][aco_1697][aco_1048][aco_1691][aco_994][aco_1414][aco_994][aco_994][aco_798][aco_1137][aco_1951][aco_994][aco_612][aco_1181][aco_994][aco_107][aco_1697][aco_1414][aco_1115][aco_1697][aco_994][aco_1697][aco_1048][aco_1026][aco_2017][aco_1115][aco_1951][aco_1697][aco_1951][aco_107][aco_798][aco_1951][aco_1048][aco_1414][aco_1048][aco_1181][aco_315][aco_1691][aco_1951][aco_994][aco_1697][aco_1691][aco_1048][aco_2017][aco_1026][aco_1697][aco_1181][aco_994][aco_1137][aco_612][aco_107][aco_1026][aco_1951][aco_107][aco_1181][aco_994][aco_612][aco_1026][aco_994][aco_1691][aco_1691][aco_1697][aco_1691][aco_1951][aco_1048][aco_107][aco_1414][aco_1048][aco_1697][aco_994][aco_1181][aco_1691][aco_1951][aco_1048][aco_1048][aco_1048][aco_1414][aco_974][aco_798][aco_1691][aco_1181][aco_1697][aco_1697][aco_1137][aco_612][aco_1048][aco_1951][aco_798][aco_1697][aco_1137][aco_1048][aco_1048][aco_471][aco_1414][aco_1181][aco_2017][aco_1181][aco_1115][aco_471][aco_994][aco_1181][aco_60][aco_1951][aco_1048][aco_1951][aco_1137][aco_1951][aco_107][aco_1697][aco_1697][aco_798][aco_1951][aco_798][aco_1691][aco_471][aco_1414][aco_994][aco_798][aco_1026][aco_1951][aco_1181][aco_107][aco_994][aco_1697][aco_798][aco_1691][aco_1181][aco_677][aco_994][aco_677][aco_612][aco_1048][aco_1048][aco_677][aco_677][aco_1414][aco_677][aco_1026][aco_677][aco_1414][aco_1181][aco_1697][aco_1048][aco_1691][aco_677][aco_2017][aco_1951][aco_1951][aco_1048][aco_107][aco_612][aco_1048][aco_2017][aco_1048][aco_2017][aco_994][aco_2017][aco_677][aco_1048][aco_2017][aco_1048][aco_1048][aco_1697][aco_1137][aco_994][aco_1414][aco_1414][aco_107][aco_471][aco_1697][aco_1048][aco_798][aco_107][aco_1951][aco_994][aco_1697][aco_107][aco_142][aco_2017][aco_1697][aco_1697][aco_1691][aco_994][aco_1048][aco_1697][aco_1181][aco_1951][aco_1691][aco_1697][aco_315][aco_798][aco_677][aco_1414][aco_1048][aco_1048][aco_315][aco_1048][aco_1697][aco_315][aco_1691][aco_1048][aco_994][aco_994][aco_2017][aco_1048][aco_994][aco_1697][aco_1048][aco_1414][aco_1697][aco_1691][aco_2017][aco_60][aco_1026][aco_107][aco_1414][aco_1181][aco_1691][aco_1048][aco_1414][aco_1414][aco_1951][aco_677][aco_677][aco_1697][aco_107][aco_1697][aco_677][aco_107][aco_107][aco_1691][aco_677][aco_1697][aco_1691][aco_1951][aco_1691][aco_1697][aco_2017][aco_994][aco_1691][aco_994][aco_994][aco_1181][aco_1115][aco_471][aco_1414][aco_798][aco_1414][aco_2017][aco_2017][aco_1048][aco_471][aco_107][aco_798][aco_677][aco_612][aco_2017][aco_1181][aco_1181][aco_1048][aco_60][aco_60][aco_60][aco_994][aco_798][aco_994][aco_994][aco_677][aco_1414][aco_1048][aco_798][aco_107][aco_1691][aco_107][aco_1181][aco_798][aco_1691][aco_1137][aco_994][aco_1137][aco_994][aco_1951][aco_1691][aco_994][aco_2017][aco_107][aco_1414][aco_1697][aco_107][aco_1697][aco_2017][aco_1691][aco_677][aco_1137][aco_1181][aco_612][aco_994][aco_1414][aco_1697][aco_1697][aco_612][aco_1115][aco_994][aco_1181][aco_1697][aco_1691][aco_798][aco_1414][aco_612][aco_1691][aco_1691][aco_1181][aco_798][aco_1414][aco_798][aco_798][aco_107][aco_1414][aco_677][aco_1691][aco_1414][aco_1951][aco_107][aco_798][aco_1951][aco_1181][aco_1414][aco_1048][aco_107][aco_798][aco_677][aco_1697][aco_677][aco_1181][aco_1414][aco_1697][aco_107][aco_798][aco_1414][aco_1697][aco_1697][aco_1181][aco_1048][aco_2017][aco_1137][aco_677][aco_798][aco_677][aco_677][aco_142][aco_1048][aco_677][aco_471][aco_1414][aco_994][aco_1691][aco_1951][aco_798][aco_107][aco_1048][aco_1181][aco_1048][aco_612][aco_2017][aco_1414][aco_1048][aco_1414][aco_677][aco_1691][aco_1691][aco_1697][aco_1048][aco_1697][aco_1697][aco_1048][aco_798][aco_2017][aco_994][aco_1048][aco_994][aco_1048][aco_677][aco_612][aco_1691][aco_677][aco_1691][aco_2017][aco_1048][aco_1414][aco_1951][aco_2017][aco_1181][aco_1697][aco_107][aco_315][aco_677][aco_1691][aco_994][aco_2017][aco_1697][aco_1137][aco_1697][aco_994][aco_994][aco_1691][aco_1697][aco_1951][aco_1048][aco_994][aco_1951][aco_1048][aco_1048][aco_1414][aco_1697][aco_677][aco_1951][aco_994][aco_1414][aco_994][aco_1951][aco_677][aco_1137][aco_1181][aco_798][aco_1691][aco_1181][aco_2017][aco_1691][aco_1048][aco_1048][aco_471][aco_1137][aco_1697][aco_1048][aco_994][aco_1697][aco_1691][aco_798][aco_1697][aco_1048][aco_107][aco_1691][aco_1697][aco_1181][aco_1048][aco_1181][aco_1951][aco_677][aco_1697][aco_1414][aco_1048][aco_1414][aco_1697][aco_677][aco_2017][aco_1048][aco_798][aco_677][aco_1414][aco_1414][aco_798][aco_1048][aco_1048][aco_60][aco_1697][aco_1697][aco_107][aco_1414][aco_1951][aco_1414][aco_677][aco_677][aco_1181][aco_471][aco_1048][aco_2017][aco_1414][aco_107][aco_994][aco_994][aco_1414][aco_1414][aco_1137][aco_1181][aco_994][aco_1691][aco_1048][aco_1414][aco_1951][aco_612][aco_1048][aco_1181][aco_2017][aco_1697][aco_612][aco_1691][aco_1048][aco_1691][aco_677][aco_994][aco_1137][aco_1691][aco_994][aco_1048][aco_677][aco_2017][aco_1697][aco_1181][aco_994][aco_994][aco_1697][aco_1181][aco_1181][aco_798][aco_1697][aco_677][aco_1414][aco_994][aco_1181][aco_1951][aco_1414][aco_994][aco_107][aco_1181][aco_1181][aco_798][aco_1691][aco_1181][aco_471][aco_1048][aco_107][aco_1181][aco_1414][aco_2017][aco_1691][aco_994][aco_1115][aco_1691][aco_1697][aco_677][aco_1048][aco_1691][aco_1048][aco_1137][aco_1414][aco_1691][aco_1048][aco_994][aco_1414][aco_974][aco_677][aco_798][aco_1697][aco_1697][aco_798][aco_1414][aco_1697][aco_1691][aco_994][aco_1048][aco_994][aco_1691][aco_1697][aco_1181][aco_1414][aco_994][aco_1691][aco_994][aco_1697][aco_798][aco_994][aco_994][aco_1048][aco_1951][aco_677][aco_1026][aco_1048][aco_1697][aco_107][aco_1115][aco_1048][aco_1048][aco_1181][aco_994][aco_107][aco_1026][aco_1115][aco_1414][aco_1951][aco_1048][aco_1414][aco_994][aco_1048][aco_677][aco_1697][aco_471][aco_2017][aco_2017][aco_798][aco_2017][aco_1048][aco_798][aco_1115][aco_1048][aco_107][aco_1048][aco_1691][aco_994][aco_677][aco_1691][aco_798][aco_677][aco_2017]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TTSTokenizer()\n",
    "print(tokenizer.decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
