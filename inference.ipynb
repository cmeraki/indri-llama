{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c9fb8cf-0001-40c7-ac55-aaa9aff6048b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 10:56:00.292601: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-18 10:56:00.300439: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734499560.309602 2387918 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734499560.312496 2387918 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-18 10:56:00.323024: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Llama Tokenizer - Step 1\n",
    "from transformers import MimiModel, AutoFeatureExtractor, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TextTokenizer:\n",
    "    def __init__(self, name='Llama_tokenizer'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(name, legacy=False)\n",
    "        print(\"text vocab size\", self.tokenizer.vocab_size)\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return self.tokenizer.decode(tokens)\n",
    "    \n",
    "class MimiTokenizer:\n",
    "    def __init__(self, device):    \n",
    "        self.device = device\n",
    "        self.model = MimiModel.from_pretrained(\"kyutai/mimi\")\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(\"kyutai/mimi\", device=device)\n",
    "        self.sampling_rate = self.feature_extractor.sampling_rate\n",
    "        self.n_codebooks = 8\n",
    "        self.vocab_size = 2048\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def encode(self, waveform):\n",
    "        inputs = self.feature_extractor(raw_audio=waveform, \n",
    "                                        sampling_rate=self.sampling_rate, \n",
    "                                        return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "        output = self.model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"], num_quantizers=self.n_codebooks)\n",
    "        tokens = output.audio_codes[0].cpu().numpy()\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        assert len(tokens.shape) == 2\n",
    "        tokens = torch.tensor(np.expand_dims(tokens, axis=0)).to(self.device)\n",
    "        output = self.model.decode(tokens)\n",
    "        waveform = output.audio_values.cpu()\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "382c3946-39e1-47ca-b031-f5e4962d1afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class TTSTokenizer:\n",
    "    def __init__(self, text_tokenizer_name='tts_tokenizer', audio_tokenizer_name='tts_tokenizer'):\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(text_tokenizer_name, legacy=False)\n",
    "        self.audio_tokenizer = AutoTokenizer.from_pretrained(audio_tokenizer_name, legacy=False)\n",
    "        print(\"text vocab size\", self.audio_tokenizer.vocab_size)\n",
    "\n",
    "    def encode(self, input_data, add_special_tokens=True):\n",
    "        if isinstance(input_data, str):\n",
    "            encoded_tokens = self.text_tokenizer.encode(\n",
    "                input_data, \n",
    "                return_tensors='pt', \n",
    "                add_special_tokens=add_special_tokens\n",
    "            )\n",
    "            return encoded_tokens\n",
    "        elif isinstance(input_data, list) and all(isinstance(item, str) for item in input_data):\n",
    "            encoded_tokens = self.audio_tokenizer.encode(\n",
    "                input_data, \n",
    "                return_tensors='pt', \n",
    "                add_special_tokens=add_special_tokens\n",
    "            )\n",
    "            return encoded_tokens\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a string or a list of strings\")\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        if not isinstance(tokens, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch tensor of tokens\")\n",
    "        \n",
    "        try:\n",
    "            decoded_text = self.text_tokenizer.decode(tokens)\n",
    "            return decoded_text\n",
    "        except:\n",
    "            try:\n",
    "                decoded_tokens = self.audio_tokenizer.decode(tokens)\n",
    "                return torch.tensor(decoded_tokens)\n",
    "            except:\n",
    "                raise ValueError(\"Unable to decode the provided tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33b3f3e0-9bff-4a14-9ff3-14cc591074f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text vocab size 128000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meraki/miniconda3/lib/python3.11/site-packages/transformers/models/mimi/modeling_mimi.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
     ]
    }
   ],
   "source": [
    "tts_tokenizer = TTSTokenizer()\n",
    "mimi_tokenizer = MimiTokenizer(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "163c92b8-6c71-4f3b-98ed-c15b55a200d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2387918/1927633890.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('models/llama_model_epoch_100.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Llama(\n",
       "  (tok_embeddings): Embedding(144646, 1024)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (wk): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (wv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "        (w2): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (w3): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=1024, out_features=144646, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from llama_model import Llama, LlamaConfig\n",
    "\n",
    "config = LlamaConfig(\n",
    "    dim=1024,  \n",
    "    n_layers=12,\n",
    "    n_heads=16,\n",
    "    vocab_size=144646,  \n",
    "    max_seq_len=2048,  \n",
    "    multiple_of=2048,\n",
    "    use_scaled_rope=True\n",
    ")\n",
    "model = Llama(config).cuda()\n",
    "state_dict = torch.load('models/llama_model_epoch_100.pth')\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a15d2677-763b-4d34-8eb9-f1225e797482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_output_tokens(model, input_tokens, max_new_tokens, temperature=0.8, top_k=50, stop_token=144644):\n",
    "    idx = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0).cuda()\n",
    "    start_pos = 0\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer.attention.cache_k = torch.zeros_like(layer.attention.cache_k)\n",
    "        layer.attention.cache_v = torch.zeros_like(layer.attention.cache_v)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx if idx.size(1) <= model.config.max_seq_len else idx[:, -model.config.max_seq_len:]        \n",
    "        h = model.tok_embeddings(idx_cond)        \n",
    "        freqs_cis = model._prepare_rotary_embeddings(h, idx_cond.size(1))        \n",
    "        seqlen = idx_cond.size(1)\n",
    "        mask = torch.triu(torch.full((seqlen, seqlen), float('-inf'), device=h.device), diagonal=1)\n",
    "        \n",
    "        for layer in model.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        \n",
    "        logits = model.norm(h)\n",
    "        logits = model.output(logits)[:, -1, :] / temperature\n",
    "        \n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = float('-inf')\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        if stop_token is not None and idx_next.item() == stop_token:\n",
    "            break\n",
    "        \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        start_pos += 1\n",
    "\n",
    "    generated_tokens = idx[:, len(input_tokens):]\n",
    "    return generated_tokens.squeeze().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25c4bbc2-f25a-4f77-90a7-41e60f1d13b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([129106, 141544, 142464,  ..., 130207, 129953, 129670])\n"
     ]
    }
   ],
   "source": [
    "input_token = [128000,  39628,     11,    304,    279,   1193,   5647,    449,    902,\n",
    "           584,    527,    520,   3118,  11920,     11,  44642,    505,   1455,\n",
    "           422,    539,    505,    682,    279,  19071,    323,  44948,  15609,\n",
    "           304,    279,  68033, 144642, 144645, 144641]\n",
    "output = generate_output_tokens(model, input_token, max_new_tokens=1024)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe65d64-c010-4991-87c0-a77abb0639ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[aco_850][aco_13288][aco_14208][aco_11413][aco_8537][aco_4973][aco_5623][aco_4847][aco_3327][aco_3494][aco_3035][aco_2369][aco_798][aco_3865][aco_1697][aco_107][aco_107][aco_1414][aco_798][aco_994][aco_798][aco_471][aco_677][aco_60][aco_1414][aco_798][aco_2017][aco_1414][aco_2017][aco_1414][aco_677][aco_677][aco_2017][aco_1691][aco_612][aco_677][aco_1414][aco_1697][aco_1181][aco_1691][aco_994][aco_1697][aco_1048][aco_798][aco_1951][aco_1951][aco_107][aco_1048][aco_107][aco_612][aco_471][aco_1691][aco_107][aco_1048][aco_1951][aco_1181][aco_1181][aco_142][aco_1697][aco_994][aco_1697][aco_994][aco_471][aco_612][aco_1181][aco_107][aco_1048][aco_1691][aco_1048][aco_60][aco_612][aco_798][aco_1026][aco_994][aco_1048][aco_994][aco_1691][aco_677][aco_107][aco_1697][aco_1181][aco_1181][aco_1697][aco_1691][aco_1414][aco_2017][aco_994][aco_60][aco_1137][aco_1181][aco_1697][aco_1048][aco_974][aco_1691][aco_994][aco_107][aco_994][aco_1697][aco_994][aco_1181][aco_471][aco_994][aco_612][aco_994][aco_994][aco_1048][aco_994][aco_1048][aco_1951][aco_994][aco_1414][aco_471][aco_1697][aco_994][aco_1048][aco_798][aco_798][aco_1181][aco_677][aco_107][aco_1697][aco_2017][aco_1691][aco_2017][aco_471][aco_471][aco_1697][aco_1048][aco_2017][aco_1414][aco_994][aco_1048][aco_677][aco_1414][aco_1048][aco_1697][aco_1691][aco_1414][aco_1691][aco_1414][aco_1951][aco_107][aco_1048][aco_677][aco_1414][aco_994][aco_1697][aco_1697][aco_1691][aco_1181][aco_677][aco_1414][aco_1691][aco_1697][aco_1048][aco_2017][aco_107][aco_1048][aco_1691][aco_994][aco_1691][aco_2017][aco_798][aco_1181][aco_2017][aco_471][aco_994][aco_1137][aco_1048][aco_1697][aco_1697][aco_1697][aco_1026][aco_1048][aco_677][aco_677][aco_107][aco_142][aco_1048][aco_994][aco_1048][aco_1414][aco_1414][aco_1691][aco_1691][aco_1697][aco_1691][aco_1951][aco_1137][aco_1048][aco_1181][aco_315][aco_1697][aco_1181][aco_1691][aco_1181][aco_1115][aco_107][aco_1137][aco_1697][aco_1048][aco_1137][aco_1414][aco_677][aco_1697][aco_798][aco_1691][aco_2017][aco_107][aco_1951][aco_1414][aco_1048][aco_1048][aco_994][aco_1691][aco_2017][aco_798][aco_107][aco_1181][aco_994][aco_107][aco_1951][aco_1137][aco_471][aco_1691][aco_2017][aco_1691][aco_1697][aco_1697][aco_677][aco_1181][aco_2017][aco_1951][aco_60][aco_994][aco_994][aco_471][aco_994][aco_2017][aco_1048][aco_1048][aco_677][aco_471][aco_1697][aco_1697][aco_994][aco_677][aco_1026][aco_994][aco_1951][aco_1697][aco_1697][aco_1691][aco_107][aco_677][aco_1414][aco_1048][aco_471][aco_1697][aco_994][aco_471][aco_1181][aco_1697][aco_1115][aco_1048][aco_1697][aco_1414][aco_107][aco_994][aco_612][aco_1414][aco_612][aco_994][aco_1048][aco_2017][aco_1697][aco_677][aco_1048][aco_1115][aco_471][aco_1951][aco_1951][aco_994][aco_1048][aco_1691][aco_974][aco_1181][aco_1691][aco_1048][aco_994][aco_1414][aco_677][aco_107][aco_1048][aco_1026][aco_1697][aco_994][aco_798][aco_1137][aco_994][aco_107][aco_1181][aco_1048][aco_1697][aco_1697][aco_1691][aco_1048][aco_1691][aco_994][aco_1697][aco_677][aco_1691][aco_107][aco_315][aco_1691][aco_1691][aco_1181][aco_1414][aco_1691][aco_2017][aco_1691][aco_677][aco_798][aco_1697][aco_798][aco_1181][aco_1048][aco_1697][aco_1414][aco_1951][aco_677][aco_994][aco_1691][aco_1414][aco_1691][aco_677][aco_1691][aco_1048][aco_677][aco_1048][aco_994][aco_994][aco_1414][aco_1691][aco_612][aco_1691][aco_1181][aco_1951][aco_2017][aco_994][aco_1048][aco_612][aco_994][aco_677][aco_994][aco_994][aco_974][aco_677][aco_1691][aco_1026][aco_1951][aco_994][aco_994][aco_994][aco_1951][aco_1048][aco_677][aco_994][aco_994][aco_1691][aco_1691][aco_1697][aco_798][aco_1181][aco_1697][aco_677][aco_994][aco_677][aco_1697][aco_1697][aco_1691][aco_994][aco_1697][aco_1048][aco_1414][aco_1691][aco_1048][aco_2017][aco_1691][aco_107][aco_677][aco_107][aco_1181][aco_612][aco_974][aco_2017][aco_1181][aco_107][aco_1181][aco_1414][aco_1115][aco_994][aco_1048][aco_1414][aco_1048][aco_1414][aco_994][aco_1414][aco_677][aco_994][aco_994][aco_107][aco_798][aco_1691][aco_994][aco_1697][aco_471][aco_994][aco_1691][aco_471][aco_1691][aco_994][aco_315][aco_612][aco_1645][aco_1048][aco_994][aco_1048][aco_1697][aco_1691][aco_1048][aco_1691][aco_994][aco_2017][aco_1026][aco_1697][aco_1691][aco_994][aco_677][aco_2017][aco_1691][aco_994][aco_1181][aco_1115][aco_1115][aco_1951][aco_994][aco_1951][aco_471][aco_1414][aco_1181][aco_1048][aco_1048][aco_1697][aco_1048][aco_471][aco_315][aco_798][aco_1048][aco_471][aco_1026][aco_471][aco_315][aco_994][aco_1697][aco_1414][aco_1181][aco_798][aco_798][aco_107][aco_1858][aco_471][aco_994][aco_1697][aco_1181][aco_798][aco_1048][aco_994][aco_677][aco_1697][aco_107][aco_1697][aco_1697][aco_1697][aco_798][aco_994][aco_994][aco_1951][aco_1691][aco_1048][aco_1697][aco_1697][aco_107][aco_1691][aco_1048][aco_994][aco_612][aco_1048][aco_471][aco_1697][aco_1115][aco_1691][aco_1414][aco_1691][aco_1691][aco_1691][aco_107][aco_2017][aco_1048][aco_994][aco_994][aco_1697][aco_798][aco_1697][aco_994][aco_1137][aco_994][aco_1414][aco_1137][aco_1697][aco_1181][aco_798][aco_1951][aco_1181][aco_1691][aco_2017][aco_1048][aco_471][aco_994][aco_471][aco_1181][aco_1697][aco_1697][aco_1951][aco_1697][aco_994][aco_1697][aco_994][aco_1697][aco_677][aco_1691][aco_994][aco_612][aco_1048][aco_798][aco_1691][aco_1181][aco_612][aco_1048][aco_1691][aco_1697][aco_107][aco_471][aco_1697][aco_1697][aco_1026][aco_994][aco_994][aco_994][aco_994][aco_1414][aco_1115][aco_1691][aco_1414][aco_1048][aco_1414][aco_1697][aco_1691][aco_1697][aco_677][aco_1414][aco_1048][aco_107][aco_994][aco_2017][aco_612][aco_1697][aco_612][aco_1691][aco_798][aco_1691][aco_798][aco_798][aco_612][aco_1181][aco_677][aco_1181][aco_1181][aco_2017][aco_798][aco_994][aco_471][aco_1048][aco_1691][aco_1137][aco_107][aco_1048][aco_1048][aco_798][aco_1048][aco_994][aco_994][aco_677][aco_2017][aco_994][aco_1691][aco_994][aco_1691][aco_1181][aco_1414][aco_1026][aco_1414][aco_1181][aco_994][aco_471][aco_994][aco_994][aco_1026][aco_994][aco_994][aco_1048][aco_1697][aco_1691][aco_107][aco_612][aco_994][aco_1691][aco_1951][aco_1951][aco_994][aco_1697][aco_1951][aco_1181][aco_1414][aco_1951][aco_1048][aco_994][aco_1048][aco_994][aco_1697][aco_1951][aco_677][aco_994][aco_677][aco_1414][aco_1181][aco_1048][aco_1414][aco_1414][aco_798][aco_798][aco_1048][aco_107][aco_994][aco_994][aco_1414][aco_1951][aco_1414][aco_798][aco_1115][aco_994][aco_1048][aco_994][aco_1697][aco_1697][aco_471][aco_1691][aco_471][aco_1414][aco_994][aco_2017][aco_1697][aco_107][aco_107][aco_798][aco_1951][aco_677][aco_1414][aco_1048][aco_471][aco_1048][aco_1048][aco_677][aco_1691][aco_1697][aco_1137][aco_994][aco_994][aco_1951][aco_1048][aco_1697][aco_1048][aco_677][aco_1697][aco_1048][aco_1697][aco_1048][aco_1691][aco_1858][aco_798][aco_994][aco_1137][aco_1414][aco_1414][aco_1414][aco_798][aco_1414][aco_1697][aco_994][aco_1048][aco_1048][aco_1414][aco_107][aco_798][aco_1414][aco_1414][aco_1181][aco_107][aco_1414][aco_1137][aco_107][aco_974][aco_2017][aco_677][aco_1048][aco_1697][aco_107][aco_1697][aco_107][aco_994][aco_1697][aco_994][aco_315][aco_1697][aco_1697][aco_1137][aco_612][aco_1697][aco_994][aco_1414][aco_60][aco_798][aco_1048][aco_2017][aco_974][aco_1414][aco_1691][aco_1414][aco_1181][aco_1048][aco_994][aco_1048][aco_1414][aco_471][aco_677][aco_107][aco_1697][aco_1691][aco_1414][aco_1181][aco_107][aco_1414][aco_1181][aco_677][aco_1414][aco_142][aco_994][aco_1414][aco_1691][aco_994][aco_1697][aco_994][aco_994][aco_1026][aco_1951][aco_1181][aco_107][aco_1181][aco_1414][aco_1181][aco_1691][aco_1181][aco_1048][aco_107][aco_677][aco_677][aco_1414][aco_798][aco_142][aco_1181][aco_677][aco_107][aco_1048][aco_1048][aco_677][aco_994][aco_107][aco_1691][aco_1048][aco_1414][aco_1048][aco_677][aco_1115][aco_994][aco_1181][aco_1697][aco_1697][aco_677][aco_142][aco_1181][aco_1181][aco_1137][aco_1048][aco_1697][aco_994][aco_1414][aco_107][aco_2017][aco_107][aco_1048][aco_677][aco_107][aco_798][aco_798][aco_798][aco_1691][aco_1048][aco_994][aco_2017][aco_1414][aco_798][aco_1697][aco_1048][aco_2017][aco_1697][aco_107][aco_471][aco_994][aco_798][aco_1951][aco_1691][aco_1691][aco_1048][aco_1697][aco_1691][aco_1181][aco_107][aco_1691][aco_1691][aco_2017][aco_677][aco_1691][aco_1697][aco_2017][aco_1137][aco_994][aco_1697][aco_994][aco_1951][aco_1181][aco_471][aco_1181][aco_1115][aco_1115][aco_107][aco_107][aco_1137][aco_1414][aco_994][aco_107][aco_1697][aco_677][aco_1048][aco_1951][aco_1115][aco_471][aco_994][aco_1414][aco_994][aco_1691][aco_1697][aco_1691][aco_1026][aco_994][aco_677][aco_2017][aco_994][aco_1691][aco_1181][aco_1697][aco_107][aco_1181][aco_1048][aco_471][aco_1697][aco_1414][aco_1691][aco_1115][aco_677][aco_1951][aco_677][aco_1181][aco_1951][aco_994][aco_1691][aco_1048][aco_1048][aco_1181][aco_1181][aco_1691][aco_471][aco_107][aco_107][aco_1414][aco_1691][aco_1697][aco_1691][aco_1048][aco_471][aco_1048][aco_107][aco_994][aco_994][aco_798][aco_1951][aco_1691][aco_994][aco_471][aco_1697][aco_677][aco_994][aco_798][aco_1026][aco_1691][aco_107][aco_1181][aco_1414][aco_677][aco_1414][aco_1048][aco_1137][aco_1697][aco_994][aco_677][aco_1691][aco_994][aco_1048][aco_798][aco_1951][aco_994][aco_1048][aco_994][aco_1414][aco_1697][aco_798][aco_994][aco_1691][aco_1951][aco_1181][aco_471][aco_1048][aco_1048][aco_1697][aco_107][aco_1697][aco_1691][aco_798][aco_1026][aco_1691][aco_2017][aco_1697][aco_1414][aco_677][aco_1414][aco_1048][aco_1697][aco_994][aco_1181][aco_1691][aco_1137][aco_1691][aco_1697][aco_1048][aco_315][aco_994][aco_798][aco_1691][aco_994][aco_994][aco_2017][aco_1691][aco_1026][aco_1048][aco_1697][aco_1414][aco_994][aco_1951][aco_1414][aco_1691][aco_107][aco_994][aco_1691][aco_1048][aco_1181][aco_1697][aco_1414][aco_2017][aco_1048][aco_1048][aco_1691][aco_107][aco_612][aco_1697][aco_107][aco_107][aco_1181][aco_107][aco_1691][aco_677][aco_1414][aco_1048][aco_1951][aco_1697][aco_1414]\n"
     ]
    }
   ],
   "source": [
    "print(tts_tokenizer.decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "000117f6-4783-43ff-8fa3-6eadfc5039ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition[convert][spkr_unk][mimi][aco_739][aco_2069][aco_4578][aco_7094][aco_9825][aco_10563][aco_12732][aco_15193][aco_316][aco_3016][aco_4782][aco_7020][aco_8296][aco_11160][aco_13609][aco_14266][aco_861][aco_3341][aco_5150][aco_5890][aco_8066][aco_11033][aco_13667][aco_15729][aco_813][aco_2930][aco_5406][aco_6964][aco_8256][aco_11597][aco_13504][aco_14154][aco_1364][aco_3587][aco_4865][aco_6558][aco_9180][aco_10131][aco_12828][aco_14188]<|reserved_special_token_72|>[aco_2746][aco_4782][aco_6891][aco_8821][aco_11409][aco_13641][aco_14496][aco_878][aco_2947][aco_4668][aco_6412][aco_8292][aco_11746][aco_12153][aco_15195][aco_459][aco_1964][aco_5593][aco_6969][aco_8101][aco_11519][aco_12204][aco_15340][aco_1394][aco_3681][aco_3870][aco_6289][aco_8677][aco_10821][aco_13681][aco_15608][aco_1276][aco_3383][aco_4223][aco_6579][aco_9627][aco_9990][aco_13718][aco_14945][aco_681][aco_3568][aco_5155][aco_6128][aco_7992][aco_9990][aco_13309][aco_15693][aco_821][aco_2067][aco_4119][aco_5891][aco_9813][aco_10754][aco_13256][aco_14698][aco_1096][aco_2130][aco_5604][aco_6352][aco_9892][aco_11921][aco_13432][aco_14983][aco_493][aco_2699][aco_5525][aco_7682][aco_9729][aco_11877][aco_13014][aco_15686][aco_321][aco_2035][aco_4989][aco_6002][aco_9672][aco_10331][aco_13008][aco_14648][aco_414][aco_2903][aco_4247][aco_6892][aco_8658][aco_11504][aco_12572][aco_14219][aco_1445][aco_2232][aco_5042][aco_6157][aco_8453][aco_10242][aco_12479][aco_14440][aco_1506][aco_2947][aco_4736][aco_7771][aco_8146][aco_9990][aco_13457][aco_14741][aco_314][aco_3141][aco_5126][aco_6738][aco_7939][aco_10757][aco_13819][aco_14231][aco_1212][aco_2488][aco_4798][aco_6787][aco_8407][aco_11917][aco_13407][aco_14351]<|reserved_special_token_149|>[aco_3788][aco_4530][aco_7601][aco_8401][aco_10522][aco_12477][aco_14318][aco_583][aco_2785][aco_5629][aco_6440][aco_8101][aco_10876][aco_12252][aco_16043][aco_1201][aco_2838][aco_4726][aco_7241][aco_9461][aco_11965][aco_12962][aco_14511]<|reserved_special_token_173|>[aco_3085][aco_5584][aco_7261][aco_8605][aco_11881][aco_13144][aco_14538][aco_766][aco_3416][aco_4666][aco_7305][aco_9704][aco_10613][aco_12220][aco_14899][aco_781][aco_2181][aco_4196][aco_6110][aco_8285][aco_10242][aco_12296][aco_15236][aco_811][aco_3635][aco_3904][aco_7555][aco_9385][aco_9990][aco_12684][aco_14952][aco_876][aco_2131][aco_5635][aco_7806][aco_9385][aco_11017][aco_13089][aco_15417][aco_1064][aco_2848][aco_5018][aco_6030][aco_9450][aco_10552][aco_13270][aco_15631][aco_1351][aco_2035][aco_4989][aco_6002][aco_8951][aco_10539][aco_13903][aco_15686][aco_1351][aco_3003][aco_4989][aco_6178][aco_8517][aco_11556][aco_13541][aco_15686][aco_407][aco_2035][aco_5399][aco_7236][aco_8851][aco_11427][aco_13270][aco_16088][aco_1523][aco_2035][aco_5399][aco_7236][aco_8517][aco_11427][aco_13903][aco_16088][aco_1045][aco_2035][aco_5399][aco_7236][aco_8203][aco_11427][aco_13270][aco_15824][aco_1362][aco_3509][aco_5430][aco_7664][aco_8259][aco_10312][aco_12126][aco_14398][aco_1526][aco_2424][aco_5832][aco_7132][aco_9843][aco_10908][aco_12751][aco_14580][aco_48][aco_2433][aco_5317][aco_6989][aco_9872][aco_11744][aco_12913][aco_15924][aco_963][aco_3134][aco_5810][aco_6293][aco_8584][aco_11583][aco_13770][aco_15143][aco_1384][aco_2326][aco_5371][aco_6566][aco_8254][aco_10498][aco_13515][aco_15514][aco_940][aco_2531][aco_5814][aco_7843][aco_8153][aco_10294][aco_13971][aco_14268][aco_249][aco_2527][aco_5164][aco_6858][aco_9604][aco_11848][aco_12692][aco_15915][aco_890][aco_1807][aco_4523][aco_7432][aco_9274][aco_11225][aco_13150][aco_14264][aco_528][aco_3415][aco_4194][aco_7789][aco_9410][aco_10634][aco_13444][aco_14824][aco_661][aco_3498][aco_3877][aco_6586][aco_8895][aco_11519][aco_13997][aco_15055][aco_1452][aco_2523][aco_5384][aco_7191][aco_8209][aco_10919][aco_12792][aco_14152][aco_666][aco_2130][aco_4630][aco_6637][aco_8087][aco_11324][aco_13099][aco_14338][aco_881][aco_2565][aco_4277][aco_6816][aco_7970][aco_11503][aco_12741][aco_14846][aco_1102][aco_2612][aco_5456][aco_6142][aco_9137][aco_11540][aco_12278][aco_15107]<|reserved_special_token_118|>[aco_2282][aco_4506][aco_6728][aco_8517][aco_11723][aco_12698][aco_15631][aco_576][aco_3003][aco_5537][aco_7236][aco_9672][aco_11553][aco_13270][aco_15824][aco_1487][aco_2035][aco_5399][aco_7236][aco_9672][aco_11553][aco_12857][aco_16088][aco_407][aco_2035][aco_4989][aco_6052][aco_8517][aco_11014][aco_13903][aco_15686][aco_1288][aco_2848][aco_4623][aco_7236][aco_9672][aco_11556][aco_12857][aco_16088][aco_579][aco_2685][aco_4346][aco_6793][aco_8410][aco_11357][aco_12604][aco_16013][aco_81][aco_2931][aco_4833][aco_6731][aco_8422][aco_11193][aco_12209][aco_15643][aco_1121][aco_3561][aco_5525][aco_6157][aco_8081][aco_11965][aco_13228][aco_15214][aco_1768][aco_2485][aco_5371][aco_6414][aco_9709][aco_10258][aco_13515][aco_14572][aco_996][aco_1984][aco_5430][aco_6241][aco_9836][aco_11759][aco_13904][aco_15178][aco_1004][aco_2615][aco_4960][aco_7578][aco_9324][aco_10879][aco_13607][aco_14268][aco_48][aco_2161][aco_4506][aco_6958][aco_9784][aco_11000][aco_12919][aco_14472][aco_783][aco_3569][aco_5413][aco_7352][aco_9941][aco_10538][aco_12116][aco_15266][aco_1384][aco_2440][aco_5104][aco_7869][aco_9757][aco_10657][aco_13114][aco_14742][aco_541][aco_3003][aco_5342][aco_6030][aco_8203][aco_10331][aco_13270][aco_15493][aco_6][aco_3003][aco_5399][aco_7236][aco_8276][aco_11553][aco_13270][aco_15824][aco_407][aco_2035][aco_5399][aco_7236][aco_9672][aco_11553][aco_14010][aco_16088][aco_128][aco_3141][aco_4506][aco_7585][aco_9941][aco_10538][aco_13025][aco_15948][aco_1769][aco_2581][aco_5430][aco_6410][aco_9703][aco_10783][aco_13177][aco_14923][aco_1518][aco_2723][aco_5522][aco_6485][aco_8754][aco_10550][aco_13377][aco_14810]<|reserved_special_token_170|>[aco_2424][aco_5520][aco_6621][aco_8153][aco_11321][aco_12425][aco_15602]<|reserved_special_token_121|>[aco_2995][aco_5339][aco_7577][aco_8140][aco_9990][aco_12413][aco_14342][aco_1413][aco_2077][aco_4096][aco_6622][aco_8461][aco_9990][aco_12833][aco_15526][aco_165][aco_2249][aco_4092][aco_6582][aco_9305][aco_10485][aco_14049][aco_15347][aco_1263][aco_3702][aco_4212][aco_7334][aco_8710][aco_11165][aco_13220][aco_14404][aco_766][aco_2695][aco_5450][aco_7626][aco_8265][aco_10865][aco_13324][aco_15376][aco_1306][aco_2120][aco_4057][aco_6579][aco_9385][aco_9990][aco_12447][aco_14909][aco_996][aco_3371][aco_5155][aco_6480][aco_7952][aco_11316][aco_12744][aco_14691][aco_1526][aco_2212][aco_5226][aco_6540][aco_8603][aco_11831][aco_13501][aco_14803][aco_48][aco_3310][aco_4947][aco_6916][aco_8348][aco_11053][aco_13081][aco_15530][aco_1626][aco_1823][aco_5215][aco_7076][aco_8209][aco_11618][aco_14047][aco_14252][aco_1465][aco_1969][aco_5155][aco_6951][aco_9542][aco_9990][aco_13261][aco_15498][aco_1100][aco_3732][aco_5186][aco_6793][aco_9450][aco_11039][aco_13045][aco_15170]<|reserved_special_token_88|>[aco_3507][aco_5327][aco_7626][aco_9786][aco_10921][aco_12323][aco_15492][aco_521][aco_3820][aco_5027][aco_7657][aco_8070][aco_10783][aco_12737][aco_15593][aco_249][aco_3140][aco_5661][aco_7549][aco_8226][aco_11545][aco_12342][aco_15209][aco_270][aco_3368][aco_5827][aco_7677][aco_8863][aco_10551][aco_13963][aco_15286][aco_890][aco_3270][aco_4952][aco_6633][aco_8284][aco_10353][aco_12625][aco_15704][aco_979][aco_2586][aco_5864][aco_6800][aco_7983][aco_11115][aco_13809][aco_15209][aco_725][aco_3658][aco_4663][aco_7884][aco_9275][aco_11414][aco_12124][aco_15093][aco_1452][aco_2046][aco_4278][aco_7842][aco_9018][aco_10498][aco_13794][aco_14856][aco_1217][aco_2615][aco_4804][aco_7828][aco_8968][aco_11544][aco_13729][aco_14468][aco_1102][aco_2161][aco_4163][aco_5945][aco_8776][aco_10953][aco_13992][aco_15932][aco_246][aco_2848][aco_5466][aco_6052][aco_8517][aco_11014][aco_13544][aco_16088][aco_1012][aco_3296][aco_3952][aco_6502][aco_9498][aco_10389][aco_12707][aco_15693][aco_629][aco_2475][aco_4786][aco_6420][aco_9811][aco_10433][aco_12897][aco_14262][aco_1302][aco_2306][aco_4506][aco_6891][aco_8223][aco_10249][aco_12898][aco_14698][aco_996][aco_2046][aco_5367][aco_7467][aco_8517][aco_10564][aco_13405][aco_14660][aco_1004][aco_3824][aco_4630][aco_7379][aco_9433][aco_11820][aco_13370][aco_14803][aco_48][aco_3779][aco_4498][aco_7707][aco_9132][aco_11478][aco_12074][aco_15335][aco_1375][aco_2658][aco_5407][aco_6019][aco_9468][aco_11688][aco_12851][aco_14648][aco_1309][aco_2230][aco_4466][aco_7898][aco_8473][aco_10291][aco_12784][aco_14231][stop]\n"
     ]
    }
   ],
   "source": [
    "result = torch.tensor([128000,  39628,     11,    304,    279,   1193,   5647,    449,    902,\n",
    "           584,    527,    520,   3118,  11920,     11,  44642,    505,   1455,\n",
    "           422,    539,    505,    682,    279,  19071,    323,  44948,  15609,\n",
    "           304,    279,  68033, 144642, 144645, 144641, 128995, 130325, 132834,\n",
    "        135350, 138081, 138819, 140988, 143449, 128572, 131272, 133038, 135276,\n",
    "        136552, 139416, 141865, 142522, 129117, 131597, 133406, 134146, 136322,\n",
    "        139289, 141923, 143985, 129069, 131186, 133662, 135220, 136512, 139853,\n",
    "        141760, 142410, 129620, 131843, 133121, 134814, 137436, 138387, 141084,\n",
    "        142444, 128080, 131002, 133038, 135147, 137077, 139665, 141897, 142752,\n",
    "        129134, 131203, 132924, 134668, 136548, 140002, 140409, 143451, 128715,\n",
    "        130220, 133849, 135225, 136357, 139775, 140460, 143596, 129650, 131937,\n",
    "        132126, 134545, 136933, 139077, 141937, 143864, 129532, 131639, 132479,\n",
    "        134835, 137883, 138246, 141974, 143201, 128937, 131824, 133411, 134384,\n",
    "        136248, 138246, 141565, 143949, 129077, 130323, 132375, 134147, 138069,\n",
    "        139010, 141512, 142954, 129352, 130386, 133860, 134608, 138148, 140177,\n",
    "        141688, 143239, 128749, 130955, 133781, 135938, 137985, 140133, 141270,\n",
    "        143942, 128577, 130291, 133245, 134258, 137928, 138587, 141264, 142904,\n",
    "        128670, 131159, 132503, 135148, 136914, 139760, 140828, 142475, 129701,\n",
    "        130488, 133298, 134413, 136709, 138498, 140735, 142696, 129762, 131203,\n",
    "        132992, 136027, 136402, 138246, 141713, 142997, 128570, 131397, 133382,\n",
    "        134994, 136195, 139013, 142075, 142487, 129468, 130744, 133054, 135043,\n",
    "        136663, 140173, 141663, 142607, 128157, 132044, 132786, 135857, 136657,\n",
    "        138778, 140733, 142574, 128839, 131041, 133885, 134696, 136357, 139132,\n",
    "        140508, 144299, 129457, 131094, 132982, 135497, 137717, 140221, 141218,\n",
    "        142767, 128181, 131341, 133840, 135517, 136861, 140137, 141400, 142794,\n",
    "        129022, 131672, 132922, 135561, 137960, 138869, 140476, 143155, 129037,\n",
    "        130437, 132452, 134366, 136541, 138498, 140552, 143492, 129067, 131891,\n",
    "        132160, 135811, 137641, 138246, 140940, 143208, 129132, 130387, 133891,\n",
    "        136062, 137641, 139273, 141345, 143673, 129320, 131104, 133274, 134286,\n",
    "        137706, 138808, 141526, 143887, 129607, 130291, 133245, 134258, 137207,\n",
    "        138795, 142159, 143942, 129607, 131259, 133245, 134434, 136773, 139812,\n",
    "        141797, 143942, 128663, 130291, 133655, 135492, 137107, 139683, 141526,\n",
    "        144344, 129779, 130291, 133655, 135492, 136773, 139683, 142159, 144344,\n",
    "        129301, 130291, 133655, 135492, 136459, 139683, 141526, 144080, 129618,\n",
    "        131765, 133686, 135920, 136515, 138568, 140382, 142654, 129782, 130680,\n",
    "        134088, 135388, 138099, 139164, 141007, 142836, 128304, 130689, 133573,\n",
    "        135245, 138128, 140000, 141169, 144180, 129219, 131390, 134066, 134549,\n",
    "        136840, 139839, 142026, 143399, 129640, 130582, 133627, 134822, 136510,\n",
    "        138754, 141771, 143770, 129196, 130787, 134070, 136099, 136409, 138550,\n",
    "        142227, 142524, 128505, 130783, 133420, 135114, 137860, 140104, 140948,\n",
    "        144171, 129146, 130063, 132779, 135688, 137530, 139481, 141406, 142520,\n",
    "        128784, 131671, 132450, 136045, 137666, 138890, 141700, 143080, 128917,\n",
    "        131754, 132133, 134842, 137151, 139775, 142253, 143311, 129708, 130779,\n",
    "        133640, 135447, 136465, 139175, 141048, 142408, 128922, 130386, 132886,\n",
    "        134893, 136343, 139580, 141355, 142594, 129137, 130821, 132533, 135072,\n",
    "        136226, 139759, 140997, 143102, 129358, 130868, 133712, 134398, 137393,\n",
    "        139796, 140534, 143363, 128126, 130538, 132762, 134984, 136773, 139979,\n",
    "        140954, 143887, 128832, 131259, 133793, 135492, 137928, 139809, 141526,\n",
    "        144080, 129743, 130291, 133655, 135492, 137928, 139809, 141113, 144344,\n",
    "        128663, 130291, 133245, 134308, 136773, 139270, 142159, 143942, 129544,\n",
    "        131104, 132879, 135492, 137928, 139812, 141113, 144344, 128835, 130941,\n",
    "        132602, 135049, 136666, 139613, 140860, 144269, 128337, 131187, 133089,\n",
    "        134987, 136678, 139449, 140465, 143899, 129377, 131817, 133781, 134413,\n",
    "        136337, 140221, 141484, 143470, 130024, 130741, 133627, 134670, 137965,\n",
    "        138514, 141771, 142828, 129252, 130240, 133686, 134497, 138092, 140015,\n",
    "        142160, 143434, 129260, 130871, 133216, 135834, 137580, 139135, 141863,\n",
    "        142524, 128304, 130417, 132762, 135214, 138040, 139256, 141175, 142728,\n",
    "        129039, 131825, 133669, 135608, 138197, 138794, 140372, 143522, 129640,\n",
    "        130696, 133360, 136125, 138013, 138913, 141370, 142998, 128797, 131259,\n",
    "        133598, 134286, 136459, 138587, 141526, 143749, 128262, 131259, 133655,\n",
    "        135492, 136532, 139809, 141526, 144080, 128663, 130291, 133655, 135492,\n",
    "        137928, 139809, 142266, 144344, 128384, 131397, 132762, 135841, 138197,\n",
    "        138794, 141281, 144204, 130025, 130837, 133686, 134666, 137959, 139039,\n",
    "        141433, 143179, 129774, 130979, 133778, 134741, 137010, 138806, 141633,\n",
    "        143066, 128178, 130680, 133776, 134877, 136409, 139577, 140681, 143858,\n",
    "        128129, 131251, 133595, 135833, 136396, 138246, 140669, 142598, 129669,\n",
    "        130333, 132352, 134878, 136717, 138246, 141089, 143782, 128421, 130505,\n",
    "        132348, 134838, 137561, 138741, 142305, 143603, 129519, 131958, 132468,\n",
    "        135590, 136966, 139421, 141476, 142660, 129022, 130951, 133706, 135882,\n",
    "        136521, 139121, 141580, 143632, 129562, 130376, 132313, 134835, 137641,\n",
    "        138246, 140703, 143165, 129252, 131627, 133411, 134736, 136208, 139572,\n",
    "        141000, 142947, 129782, 130468, 133482, 134796, 136859, 140087, 141757,\n",
    "        143059, 128304, 131566, 133203, 135172, 136604, 139309, 141337, 143786,\n",
    "        129882, 130079, 133471, 135332, 136465, 139874, 142303, 142508, 129721,\n",
    "        130225, 133411, 135207, 137798, 138246, 141517, 143754, 129356, 131988,\n",
    "        133442, 135049, 137706, 139295, 141301, 143426, 128096, 131763, 133583,\n",
    "        135882, 138042, 139177, 140579, 143748, 128777, 132076, 133283, 135913,\n",
    "        136326, 139039, 140993, 143849, 128505, 131396, 133917, 135805, 136482,\n",
    "        139801, 140598, 143465, 128526, 131624, 134083, 135933, 137119, 138807,\n",
    "        142219, 143542, 129146, 131526, 133208, 134889, 136540, 138609, 140881,\n",
    "        143960, 129235, 130842, 134120, 135056, 136239, 139371, 142065, 143465,\n",
    "        128981, 131914, 132919, 136140, 137531, 139670, 140380, 143349, 129708,\n",
    "        130302, 132534, 136098, 137274, 138754, 142050, 143112, 129473, 130871,\n",
    "        133060, 136084, 137224, 139800, 141985, 142724, 129358, 130417, 132419,\n",
    "        134201, 137032, 139209, 142248, 144188, 128502, 131104, 133722, 134308,\n",
    "        136773, 139270, 141800, 144344, 129268, 131552, 132208, 134758, 137754,\n",
    "        138645, 140963, 143949, 128885, 130731, 133042, 134676, 138067, 138689,\n",
    "        141153, 142518, 129558, 130562, 132762, 135147, 136479, 138505, 141154,\n",
    "        142954, 129252, 130302, 133623, 135723, 136773, 138820, 141661, 142916,\n",
    "        129260, 132080, 132886, 135635, 137689, 140076, 141626, 143059, 128304,\n",
    "        132035, 132754, 135963, 137388, 139734, 140330, 143591, 129631, 130914,\n",
    "        133663, 134275, 137724, 139944, 141107, 142904, 129565, 130486, 132722,\n",
    "        136154, 136729, 138547, 141040, 142487, 144644],dtype=torch.int32)\n",
    "\n",
    "print(tts_tokenizer.decode(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2bcd18-15f6-458f-ae9c-4b6d45ec7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deflatten_tokens(tokens, n_codebooks, per_codebook_size):\n",
    "    arr = []\n",
    "    for i in range(n_codebooks):\n",
    "        arr.append(tokens[i::n_codebooks])\n",
    "    acoustic_tokens = np.stack(arr)\n",
    "    return acoustic_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
